{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3863171,"sourceType":"datasetVersion","datasetId":2296957},{"sourceId":9007343,"sourceType":"datasetVersion","datasetId":5426455}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project: Pizza Classification and Ingredient Recognition\n\n## Context\nThis project utilizes Kaggle datasets and Python notebooks running on GPUs to train two distinct models:\n1. An image classification model to detect the presence of a pizza.\n2. A model to identify the ingredients of a pizza.\n\nThe final objective is to integrate these two models to build an application capable of retrieving a pizza image, identifying if it’s a pizza, and listing the main ingredients.\n\n---\n\n## Project Steps\n\n### 1. Retrieving the \"Pizza or Not Pizza\" Dataset\n- **Objective**: Download the \"Pizza or Not Pizza\" dataset from Kaggle for training the first model.\n- **Steps**:\n  - Import the dataset via the Kaggle API.\n  - Prepare images for training (preprocessing, resizing).\n\n### 2. Training and Optimization of the \"Pizza or Not Pizza\" Model\n- **Objective**: Train a classification model to detect whether an image contains a pizza.\n- **Steps**:\n  - Define the model architecture (e.g., CNN).\n  - Use GPUs to accelerate training.\n  - Optimize hyperparameters (e.g., learning rate, number of epochs).\n\n### 3. Model Evaluation and Visualization Tool\n- **Objective**: Evaluate the model's performance and visualize some example predictions.\n- **Steps**:\n  - Calculate performance metrics (precision, recall, F1-score).\n  - Use a notebook to visualize examples of correct and incorrect predictions.\n\n### 4. Retrieving the \"Pizza Ingredients\" Dataset\n- **Objective**: Download the pizza ingredients dataset for training the second model.\n- **Steps**:\n  - Import the dataset via Kaggle.\n  - Process the data to make it compatible with the ingredient identification model.\n  \n\n### 5. Training and Optimization of the \"Pizza Ingredients\" Model\n- **Objective**: Train a model to recognize ingredients in pizza images.\n- **Steps**:\n  - Design the model for ingredient identification. In step 4, we now have imported the dataset with labels, and we then have to build the technology to put the localization boxes on these images. In that way, we have the possibilty to measure the quality of the output of the model during training, by minimizing the distance between both reality and prediction, and the value of the label.\n  - Optimize training using GPUs on Kaggle.\n  - Adjust hyperparameters to improve prediction accuracy.\n\n### 6. Test and Visualization of Ingredient Predictions\n- **Objective**: Test the ingredient model and visualize the predictions.\n- **Steps**:\n  - **Evaluation**: Calculate performance metrics such as precision, recall, and F1-score.\n  - **Prediction Visualization**: Implement an algorithm to take an image as input and return it with labeled or segmented ingredients.\n    - **Algorithm**:\n      1. Load the trained ingredient model.\n      2. Preprocess the input image (resize, normalize, etc.).\n      3. Apply the model to predict ingredients.\n      4. Create a segmentation map based on predicted ingredients.\n      5. Overlay the segmentation map on the original image.\n      6. Return the labeled image.\n  - **Example Visualization**: Display an example image with ingredient labels.\n\n### 7. Integration of Both Models\n- **Objective**: Integrate the \"Pizza or Not Pizza\" and \"Pizza Ingredients\" models for a comprehensive classification.\n- **Steps**:\n  - Create a function that first applies the pizza detection model, followed by the ingredient model.\n  - Handle cases where the image does not contain a pizza (detected as \"not pizza\").\n\n### 8. Building a Complete Example\n- **Objective**: Set up an end-to-end workflow for pizza and ingredient recognition.\n- **Steps**:\n  - Create a function to retrieve a pizza image (from a URL or by upload).\n  - Pass the image through the model pipeline to detect pizza and list ingredients.\n  - Present the final result in a notebook, with visualizations of the intermediate steps.\n\n---\n\n## Technologies and Tools\n- **Language**: Python\n- **Frameworks**: TensorFlow or PyTorch\n- **Execution Environment**: Kaggle Notebooks with GPU support\n- **Visualization**: Matplotlib, Seaborn for results display\n- **Dataset Access**: Kaggle API\n\n---\n\n## Deliverables\n- **Kaggle Notebooks**: Containing each step of the process.\n- **Trained Models**: Exported model files for pizza detection and ingredient identification.\n- **Documentation**: Explanation of each step and results.\n\n---\n\nThis project will provide hands-on experience in training image classification models and integrating them into a complete image analysis pipeline.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Utilities for Processing the \"Pizza Toppings\" Object Detection Dataset\n\nThis notebook defines and uses a set of utility functions to streamline the loading, conversion, and visualization of data for object detection. These functions are designed to efficiently prepare images and annotations from the **Pizza Toppings Object Detection** dataset, simplifying the workflow for training an object detection model with Keras and TensorFlow. Below is an overview of the primary utilities:\n\n### Utility Functions\n\n1. **`load_imgs(path, no_of_images, img_size, do_display)`**  \n   This function loads a specified number of images from a folder, resizes each image, and optionally displays them. It returns a dictionary of images and a list of their paths, standardizing image sizes for use in a deep learning model.\n\n2. **`convert_bbox(coordinates, image_width, image_height)`**  \n   This function converts bounding box coordinates from the YOLOv8 format (center coordinates and relative width/height) to a more conventional format (top-left and bottom-right corners). This conversion is essential for visualizing annotations on images and ensuring compatibility with libraries like PIL or OpenCV.\n\n3. **`load_annotations(path, labels, no_of_annotations, do_display)`**  \n   This function loads annotations from text files and associates them with the dataset’s label definitions. It processes and converts coordinates, returning a dictionary of annotations for each image. This utility links each image with its bounding box information, simplifying data preparation for model training.\n\n4. **`plot_image_annotations(image_paths, annotations, label_colors, target_size, do_display, text_display, font_scale, alpha)`**  \n   This function displays images with their annotations by drawing bounding boxes around detected objects. It uses predefined colors for each label and offers customization options like text scaling and box transparency. This utility is key for visually inspecting the annotated data and ensuring data quality before training.\n   \n5. **`plot_model_evaluation(history)`**\n    This function visualizes the performance of a model by plotting training and validation losses for bounding box and classification outputs. It helps in understanding model convergence and monitoring overfitting or underfitting trends over epochs.\n\n\nThese utilities provide a structured way to load, process, and handle images and annotations, making the data workflow efficient and consistent for object detection model training.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image, ImageDraw, ImageFont\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import History\nfrom typing import Dict, List, Tuple\n\n# Global Variables\n\nINPUT_SHAPE: Tuple[int, int, int] = (512, 512, 3)\nIMG_SIZE: Tuple[int, int] = (512, 512)\nSAVE_MODEL: bool = True\n\ndef log_debug_template(msg: str = \"Cell executed\") -> None:\n    print(\"-\" * 100)\n    print(\"\")\n    print(\" \" * 40 + msg)\n    print(\"\")\n    print(\"-\" * 100)\n\ndef load_imgs(path: str, no_of_images: int, img_size: Tuple[int, int] = IMG_SIZE, do_display: bool = False) -> Tuple[Dict[int, Image.Image], List[str]]:\n    \"\"\"Function which loads images from the given path\n\n    Args:\n        path (str): path to the folder with images.\n        no_of_images (int): number of images to load.\n        img_size (tuple, optional): size of the image. Defaults to IMG_SIZE.\n        do_display (bool): flag to display images. Defaults to False.\n\n    Returns:\n        images (dict): dictionary with images\n        image_paths (list): list of image paths\n    \"\"\"\n    # Declaring necessary variables\n    images = {}\n    image_paths = []\n\n    # Iterating through all images in the given path\n    for img_no, img_name in enumerate(os.listdir(path)):\n        # Breaking from loop if no_of_images is reached\n        if img_no == no_of_images:\n            break\n\n        # Loading, resizing and storing images in a dictionary\n        image_paths.append(os.path.join(path, img_name))\n        img = Image.open(os.path.join(path, img_name))\n        img = img.resize(img_size)\n        images[img_no] = img\n\n        # Displaying images if do_display is True\n        if do_display:\n            print('\\033[1m' + 'Image {}  Image name: {}'.format(img_no+1, img_name) + '\\033[0m')\n            display(img)\n    \n    # Returning images and image paths\n    return images, image_paths\n\n\ndef load_annotations(path: str, labels: Dict[int, str], no_of_annotations: int, do_display: bool = False) -> Dict[str, List[Dict[str, List[float]]]]:\n    \"\"\"Function which loads annotations from the given path\n\n    Args:\n        path (str): path to the folder with annotations.\n        labels (dict): dictionary with labels and their corresponding string names.\n        no_of_annotations (int): number of annotations to load.\n        do_display (bool): flag to display annotations. Defaults to False.\n\n    Returns:\n        annotations (dict): dictionary with annotations.\n    \"\"\"\n    # Declaring necessary variables\n    annotations = {}\n\n    # Iterating over all files in the directory\n    for filename in os.listdir(path):\n        # Breaking from loop if no_of_annotations is reached\n        if len(annotations) == no_of_annotations:\n            break\n\n        # Displaying filename if do_display is True\n        if do_display:\n            print('\\033[35m' + 'Filename: {}'.format(filename) + '\\033[0m')\n\n        # Constructing the full file path\n        file_path = os.path.join(path, filename)\n\n        # Removing the final file extension from the filename\n        filename = filename.rsplit('.', 1)[0]\n        \n        # Declaring empty list for the current filename\n        annotations[filename] = []\n\n        # Opening the file and reading its contents\n        with open(file_path, 'r') as file:\n        # Iterating through each line in the file\n            for line in file:\n                # Splitting the line into label and coordinates\n                parts = line.strip().split(' ')\n                label = labels[int(parts[0])]\n                coordinates = [float(coord) for coord in parts[1:]]\n\n                # Storing the data in the dictionary\n                annotations[filename].append({label: coordinates})\n\n                # Displaying annotations if do_display is True\n                if do_display:\n                    print('\\033[33m' + 'Label:' + '\\033[0m' + ' {}'.format(label) + '\\033[32m' + ' Coordinates:' + '\\033[0m' + ' {}'.format(coordinates))\n        \n        if do_display:\n            print()\n\n    # Returning annotations\n    return annotations\n\ndef convert_bbox(coordinates: List[float], image_width: int, image_height: int) -> List[int]:\n    \"\"\"Function to convert bounding box coordinates from YOLOv8 format to PIL format\n\n    Args:\n        coordinates (list): list of bounding box coordinates in YOLOv8 format [x_center, y_center, width, height]\n        image_width (int): width of the original image\n        image_height (int): height of the original image\n\n    Returns:\n        new_coordinates (list): list of bounding box coordinates in PIL format [x1, y1, x2, y2]\n    \"\"\"\n    # Unpacking coordinates from YOLOv8 format\n    x_center, y_center, width, height = coordinates\n\n    # Calculating new width and height\n    new_width = width * image_width\n    new_height = height * image_height\n\n    # Calculating new center\n    new_x_center = x_center * image_width\n    new_y_center = y_center * image_height\n\n    # Calculating new coordinates\n    new_x1 = int(new_x_center - (new_width / 2))\n    new_y1 = int(new_y_center - (new_height / 2))\n    new_x2 = int(new_x_center + (new_width / 2))\n    new_y2 = int(new_y_center + (new_height / 2))\n\n    # Returning new coordinates\n    return [new_x1, new_y1, new_x2, new_y2]\n\ndef plot_image_annotations(image_paths: List[str], annotations: Dict[str, List[Dict[str, List[float]]]], label_colors: Dict[str, Tuple[int, int, int]], target_size: Tuple[int, int], do_display: bool = False, text_display: bool = True, font_scale: float = 1, alpha: float = 0.4) -> None:\n    \"\"\"Function which plots images with annotations\n\n    Args:\n        image_paths (list): list of image paths.\n        annotations (dict): dictionary with annotations.\n        labels (dict): dictionary with annotation labels.\n        label_colors (dict): dictionary with annotation label colors.\n        target_size (tuple): target size of the image.\n        do_display (bool): flag to display images with annotations. Defaults to False.\n        text_display (bool): flag to display annotation labels. Defaults to True.\n        font_scale (float): font scale for the annotation labels. Defaults to 1.\n        alpha (float): alpha value for the bounding boxes. Defaults to 0.4.\n    \"\"\"\n    # Iterating through all the images\n    for img_no, image_path in enumerate(image_paths):\n        # Retrieving the image\n        image_path = image_paths[img_no]\n\n        # Reading the image\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, target_size)\n\n        # Retrieving the image name\n        image_name = image_path.split('/')[-1]\n        image_name = image_name.split('\\\\')[-1]\n\n        # Removing only last file extension from the image name\n        image_name = image_name.rsplit('.', 1)[0]\n\n        # Retrieving the image annotations\n        image_annotations = annotations[image_name]\n\n        # Drawing the bounding boxes around the image\n        for annotation in image_annotations:\n            # Retrieving the label and coordinates of the bounding box\n            label = list(annotation.keys())[0]\n            bbox = annotation[label]\n\n            # Converting the bounding box coordinates from YOLOv8 format to PIL format\n            bbox = convert_bbox(bbox, target_size[0], target_size[1])\n            \n            # Retrieving the coordinates of the bounding box\n            x1, y1, x2, y2 = bbox\n\n            # Creating a copy of the image\n            image_with_rectangle = image.copy()\n\n            # Creating a filled rectangle for the bounding box\n            cv2.rectangle(image_with_rectangle, (x1, y1), (x2, y2), label_colors[label], cv2.FILLED)\n\n            # Blending the image with the rectangle using cv2.addWeighted\n            image = cv2.addWeighted(image, 1 - alpha, image_with_rectangle, alpha, 0)\n\n            # Adding a rectangle outline to the image\n            cv2.rectangle(image, (x1, y1), (x2, y2), label_colors[label], 3)\n\n            # Adding the label to the image\n            if text_display:\n                # Setting the font\n                font = cv2.FONT_HERSHEY_SIMPLEX\n\n                # Drawing the label on the image, whilst ensuring that it doesn't go out of bounds\n                (label_width, label_height), baseline = cv2.getTextSize(label, font, font_scale, 2)\n\n                # Ensuring that the label doesn't go out of bounds\n                if y1 - label_height - baseline < 0:\n                    y1 = label_height + baseline\n                if x1 + label_width > image.shape[1]:\n                    x1 = image.shape[1] - label_width\n                if y1 + label_height + baseline > image.shape[0]:\n                    y1 = image.shape[0] - label_height - baseline\n                if x1 < 0:\n                    x1 = 0\n\n                # Creating a filled rectangle for the label background\n                cv2.rectangle(image, (x1, y1 - label_height - baseline), (x1 + label_width, y1), label_colors[label], -1)\n\n                # Adding the label text to the image\n                cv2.putText(image, label, (x1, y1 - 5), font, font_scale, (255, 255, 255), 2, cv2.LINE_AA)\n\n        # Displaying images with annotations if do_display is True\n        if do_display:\n            plt.figure(figsize=(10,10))\n            plt.imshow(image)\n            plt.axis('off')\n            plt.show()\n\ndef plot_model_evaluation(history: History) -> None:\n    \"\"\"\n    Plots the training and validation losses for bounding box and classification outputs.\n\n    Args:\n        history: History object from model training.\n    \"\"\"\n    # Plot bounding box loss\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['bounding_box_loss'], label='Train Bounding Box Loss')\n    plt.plot(history.history['val_bounding_box_loss'], label='Val Bounding Box Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Bounding Box Loss')\n    plt.legend()\n\n    # Plot classification accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['class_label_accuracy'], label='Train Classification Accuracy')\n    plt.plot(history.history['val_class_label_accuracy'], label='Val Classification Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Classification Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nlog_debug_template()","metadata":{"execution":{"iopub.status.busy":"2024-11-19T11:10:00.865516Z","iopub.execute_input":"2024-11-19T11:10:00.865846Z","iopub.status.idle":"2024-11-19T11:10:12.914324Z","shell.execute_reply.started":"2024-11-19T11:10:00.865793Z","shell.execute_reply":"2024-11-19T11:10:12.913421Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\n\n                                        Cell executed\n\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Step 4 : Retrieving the \"Pizza Ingredients\" Dataset**","metadata":{}},{"cell_type":"code","source":"# Import librairies\nimport os\nimport zipfile\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.optimizers import Adam\n# from kaggle.api.kaggle_api_extended import KaggleApi\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\nlog_debug_template()\n\n# Useless for now\n\n# AUTHENTICATION = False # Whether or not authentication is required\n\n# ## IF AUTHENTICATION REQUIRED: \n# if AUTHENTICATION:\n#     # Use Kaggle API\n#     kaggle_api = KaggleApi()\n#     # Get kaggle.json from Google Drive\n#     !pip install gdown\n\n#     # Créer le dossier .kaggle s'il n'existe pas déjà\n#     !mkdir -p /root/.kaggle\n\n#     # Download kaggle.json file from Google Drive using the file ID\n#     from kaggle_secrets import UserSecretsClient\n#     user_secrets = UserSecretsClient()\n#     file_id = user_secrets.get_secret(\"GoogleDriveKaggleJSONID\")\n\n#     destination = \"/root/.config/kaggle.json\"\n\n#     !gdown --id {file_id} -O {destination}\n#     !chmod 600 /root/.config/kaggle.json\n\n#     # Debug\n#     if os.path.exists(destination):\n#         print(f\"Le fichier 'kaggle.json' a bien été téléchargé dans : {destination}\")\n#     else:\n#         print(\"Erreur : le fichier 'kaggle.json' n'a pas été trouvé dans le répertoire spécifié.\")\n\n#     # DEBUG: Load and display config file\n#     import json\n\n#     try:\n#         with open(destination, 'r') as f:\n#             kaggle_config = json.load(f)\n#             # Vérification et affichage de l'username\n#             if 'username' in kaggle_config:\n#                 print(f\"Username trouvé dans kaggle.json : {kaggle_config['username']}\")\n#             else:\n#                 print(\"Erreur : 'username' n'a pas été trouvé dans le fichier kaggle.json.\")\n#     except Exception as e:\n#         print(f\"Erreur lors du chargement du fichier kaggle.json : {e}\")\n\n#     # Define environment variable in the repo for Kaggle config\n#     os.environ['KAGGLE_CONFIG_DIR'] = \"/root/.config\"\n\n#     kaggle_api.authenticate() \n\n\n# # Download Dataset form Kaggle\n# dataset_name = 'matthiasbartolo/pizza-toppings-object-detection'\n# download_path = './data'  # Destination repo\n\n# # Downloading files\n# kaggle_api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n\n# # Check\n# !ls ./data\n\n# # Use roboflow to decode yolo format\n# !pip install roboflow\n# from roboflow import Roboflow\n\n# rf = Roboflow(api_key=\"y2FfMokLpTlGQIlMvhja\") # Found on github, not mine\n# project = rf.workspace(\"[pol]-pizza-classification-and-semantic\").project(\"pizza-toppings-object-detection\")\n# dataset = project.version(\"7\").download(\"yolov8\", location=\"Pizza-Object-Detector-7\")\n\n\n# # Set up meta variables and labels definition\n# image_path = 'Pizza-Object-Detector-7/train/images'\n# annotation_path = 'Pizza-Object-Detector-7/train/labels'\n# LABELS = {\n#     0: \"Arugula\", 1: \"Bacon\", 2: \"Basil\", 3: \"Broccoli\", 4: \"Cheese\",\n#     5: \"Chicken\", 6: \"Corn\", 7: \"Ham\", 8: \"Mushroom\", 9: \"Olives\",\n#     10: \"Onion\", 11: \"Pepperoni\", 12: \"Peppers\", 13: \"Pineapple\", 14: \"Pizza\", 15: \"Tomatoes\"\n# }\n\n# # Loading images and annotations\n# images, image_paths = load_imgs(path=image_path, no_of_images=9000, do_display=False)\n# annotations = load_annotations(path=annotation_path, labels=LABELS, no_of_annotations=None, do_display=False)\n\n\n# log_debug_template()","metadata":{"execution":{"iopub.status.busy":"2024-11-19T11:10:40.348436Z","iopub.execute_input":"2024-11-19T11:10:40.349250Z","iopub.status.idle":"2024-11-19T11:10:40.807768Z","shell.execute_reply.started":"2024-11-19T11:10:40.349216Z","shell.execute_reply":"2024-11-19T11:10:40.806838Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\n\n                                        Cell executed\n\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The dataset is now imported in our workspace, we then have to reformat all the images to start the training. To do so, we will transform all the images using the PIL format. ","metadata":{}},{"cell_type":"code","source":"# Downloading the dataset\nimport kagglehub\npath = kagglehub.dataset_download(\"matthiasbartolo/pizza-toppings-object-detection\")","metadata":{"execution":{"iopub.status.busy":"2024-11-19T11:10:46.241002Z","iopub.execute_input":"2024-11-19T11:10:46.241571Z","iopub.status.idle":"2024-11-19T11:10:47.164134Z","shell.execute_reply.started":"2024-11-19T11:10:46.241539Z","shell.execute_reply":"2024-11-19T11:10:47.163316Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import yaml\nimport numpy as np\nimport gc\nimport os\n\ndef preprocess_data_with_utilities_bulk_and_save(\n    img_dir: str,\n    annotation_dir: str,\n    labels: Dict[int, str],\n    target_size: Tuple[int, int] = IMG_SIZE,\n    batch_size: int = 100,\n    output_dir: str = \"kaggle/working/batches\",\n    do_display: bool = False\n) -> None:\n    \"\"\"\n    Preprocesses images and annotations in batches and saves them to disk.\n\n    Args:\n        img_dir (str): Path to the folder with images.\n        annotation_dir (str): Path to the folder with annotations.\n        labels (dict): Dictionary mapping label indices to label names.\n        target_size (tuple): Target size to resize images (default is IMG_SIZE).\n        batch_size (int): Number of images to process in each batch.\n        output_dir (str): Directory to save processed batches.\n        do_display (bool): Flag to display debug information (default is False).\n    \"\"\"\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Get the full list of image paths\n    _, image_paths = load_imgs(img_dir, no_of_images=None, img_size=target_size, do_display=False)\n    total_images = len(image_paths)\n\n    # Process in batches\n    for start in range(0, total_images, batch_size):\n        end = min(start + batch_size, total_images)\n        print(f\"Processing batch {start} to {end}...\")\n\n        # Load images and annotations for the batch\n        batch_images, batch_image_paths = load_imgs(img_dir, end - start, target_size, do_display)\n        annotations = load_annotations(annotation_dir, labels, end - start, do_display)\n\n        # Temporary batch data\n        batch_X, batch_y_boxes, batch_y_classes = [], [], []\n\n        # Process each image in the batch\n        for img_no, img_path in enumerate(batch_image_paths):\n            # Load image\n            img = batch_images[img_no]  # Already resized by load_imgs\n            batch_X.append(np.array(img) / 255.0)  # Normalize to [0, 1]\n\n            # Extract image name and corresponding annotations\n            image_name = os.path.splitext(os.path.basename(img_path))[0]\n            if image_name in annotations:\n                image_annotations = annotations[image_name]\n                for annotation in image_annotations:\n                    label = list(annotation.keys())[0]  # Extract label name\n                    bbox = annotation[label]  # YOLO bounding box format\n                    bbox_converted = convert_bbox(bbox, target_size[0], target_size[1])  # Convert to [x1, y1, x2, y2]\n                    batch_y_boxes.append(bbox_converted)\n                    batch_y_classes.append(list(labels.values()).index(label))  # Map label to index\n\n        # Save the batch to disk\n        batch_id = f\"batch_{start}_{end}\"\n        np.save(os.path.join(output_dir, f\"{batch_id}_X.npy\"), np.array(batch_X))\n        np.save(os.path.join(output_dir, f\"{batch_id}_y_boxes.npy\"), np.array(batch_y_boxes))\n        np.save(os.path.join(output_dir, f\"{batch_id}_y_classes.npy\"), np.array(batch_y_classes))\n\n        print(f\"Batch {batch_id} saved to {output_dir}\")\n\n        # Free memory after processing the batch\n        del batch_X, batch_y_boxes, batch_y_classes, batch_images, annotations\n        gc.collect()\n\n    print(f\"All batches processed and saved in {output_dir}.\")\n# Paths to your dataset\nDATA_PATH = \"/kaggle/input/pizza-toppings-object-detection/Pizza Object Detector.v7i.yolov8\"\nIMG_DIR = os.path.join(DATA_PATH, \"train/images\")\nANNOTATION_DIR = os.path.join(DATA_PATH, \"train/labels\")\n\n# Load labels from data.yaml\ndata_yaml_path = os.path.join(DATA_PATH, \"data.yaml\")\nwith open(data_yaml_path, 'r') as file:\n    LABELS = yaml.safe_load(file)[\"names\"]\n\n# Define the output directory for saving batches\nOUTPUT_DIR = \"/kaggle/working/batches\"\n\n# Preprocess data and save in batches\npreprocess_data_with_utilities_bulk_and_save(\n    img_dir=IMG_DIR,\n    annotation_dir=ANNOTATION_DIR,\n    labels={idx: label for idx, label in enumerate(LABELS)},\n    target_size=IMG_SIZE,\n    batch_size=100,  # Adjust batch size based on memory\n    output_dir=OUTPUT_DIR,\n    do_display=False\n)\n\nimport numpy as np\nimport os\n\n# Directory containing saved batches\nOUTPUT_DIR = \"/kaggle/working/batches\"\n\n# Load a specific batch\nbatch_id = \"batch_0_100\"\nX_batch = np.load(os.path.join(OUTPUT_DIR, f\"{batch_id}_X.npy\"))\ny_boxes_batch = np.load(os.path.join(OUTPUT_DIR, f\"{batch_id}_y_boxes.npy\"))\ny_classes_batch = np.load(os.path.join(OUTPUT_DIR, f\"{batch_id}_y_classes.npy\"))\n\nprint(f\"Loaded batch {batch_id}:\")\nprint(f\"X shape: {X_batch.shape}\")\nprint(f\"y_boxes shape: {y_boxes_batch.shape}\")\nprint(f\"y_classes shape: {y_classes_batch.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T11:10:54.218317Z","iopub.execute_input":"2024-11-19T11:10:54.218647Z","iopub.status.idle":"2024-11-19T11:13:24.500289Z","shell.execute_reply.started":"2024-11-19T11:10:54.218614Z","shell.execute_reply":"2024-11-19T11:13:24.499599Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Processing batch 0 to 100...\nBatch batch_0_100 saved to /kaggle/working/batches\nProcessing batch 100 to 200...\nBatch batch_100_200 saved to /kaggle/working/batches\nProcessing batch 200 to 300...\nBatch batch_200_300 saved to /kaggle/working/batches\nProcessing batch 300 to 400...\nBatch batch_300_400 saved to /kaggle/working/batches\nProcessing batch 400 to 500...\nBatch batch_400_500 saved to /kaggle/working/batches\nProcessing batch 500 to 600...\nBatch batch_500_600 saved to /kaggle/working/batches\nProcessing batch 600 to 700...\nBatch batch_600_700 saved to /kaggle/working/batches\nProcessing batch 700 to 800...\nBatch batch_700_800 saved to /kaggle/working/batches\nProcessing batch 800 to 900...\nBatch batch_800_900 saved to /kaggle/working/batches\nProcessing batch 900 to 1000...\nBatch batch_900_1000 saved to /kaggle/working/batches\nProcessing batch 1000 to 1100...\nBatch batch_1000_1100 saved to /kaggle/working/batches\nProcessing batch 1100 to 1200...\nBatch batch_1100_1200 saved to /kaggle/working/batches\nProcessing batch 1200 to 1300...\nBatch batch_1200_1300 saved to /kaggle/working/batches\nProcessing batch 1300 to 1400...\nBatch batch_1300_1400 saved to /kaggle/working/batches\nProcessing batch 1400 to 1500...\nBatch batch_1400_1500 saved to /kaggle/working/batches\nProcessing batch 1500 to 1600...\nBatch batch_1500_1600 saved to /kaggle/working/batches\nProcessing batch 1600 to 1700...\nBatch batch_1600_1700 saved to /kaggle/working/batches\nProcessing batch 1700 to 1800...\nBatch batch_1700_1800 saved to /kaggle/working/batches\nProcessing batch 1800 to 1900...\nBatch batch_1800_1900 saved to /kaggle/working/batches\nProcessing batch 1900 to 2000...\nBatch batch_1900_2000 saved to /kaggle/working/batches\nProcessing batch 2000 to 2100...\nBatch batch_2000_2100 saved to /kaggle/working/batches\nProcessing batch 2100 to 2200...\nBatch batch_2100_2200 saved to /kaggle/working/batches\nProcessing batch 2200 to 2300...\nBatch batch_2200_2300 saved to /kaggle/working/batches\nProcessing batch 2300 to 2400...\nBatch batch_2300_2400 saved to /kaggle/working/batches\nProcessing batch 2400 to 2500...\nBatch batch_2400_2500 saved to /kaggle/working/batches\nProcessing batch 2500 to 2540...\nBatch batch_2500_2540 saved to /kaggle/working/batches\nAll batches processed and saved in /kaggle/working/batches.\nLoaded batch batch_0_100:\nX shape: (100, 512, 512, 3)\ny_boxes shape: (193, 4)\ny_classes shape: (193,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 5 : Training and Optimization of the \"Pizza Ingredients\" Model","metadata":{}},{"cell_type":"markdown","source":"## Why Use a Backbone?\n\nUsing a pre-trained backbone saves computation and time because we leverage the learned representations from another dataset (like ImageNet) rather than training a network from scratch.\n\n## Why ResNet50 as the Backbone in Our Case?\n\nIn our object detection task, we aim to identify different toppings on a pizza. ResNet50 is well-suited as a backbone because:\n1. **Feature-rich**: It is deep enough to capture complex and hierarchical features, essential for distinguishing various objects in a single image.\n2. **Efficient**: With optimized architecture, ResNet50 offers a good balance between accuracy and computational efficiency.\n3. **Transfer Learning Compatibility**: Pre-trained on ImageNet, it can transfer well to our object detection task, as it has already learned to recognize basic object shapes and patterns.\n\nBy freezing the backbone during training, we retain the valuable general features while focusing the training on the detection-specific layers (the bounding box head). This approach is efficient, prevents overfitting, and ensures the model can adapt well to our smaller dataset.\n","metadata":{}},{"cell_type":"code","source":"# Check the shapes of batch data\nfor batch_id in sorted([f.split('_X.npy')[0] for f in os.listdir(OUTPUT_DIR) if f.endswith('_X.npy')]):\n    X = np.load(os.path.join(OUTPUT_DIR, f\"{batch_id}_X.npy\"))\n    y_boxes = np.load(os.path.join(OUTPUT_DIR, f\"{batch_id}_y_boxes.npy\"))\n    y_classes = np.load(os.path.join(OUTPUT_DIR, f\"{batch_id}_y_classes.npy\"))\n\n    print(f\"Batch {batch_id}:\")\n    print(f\"  X shape: {X.shape}\")\n    print(f\"  y_boxes shape: {y_boxes.shape}\")\n    print(f\"  y_classes shape: {y_classes.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T08:16:56.242865Z","iopub.execute_input":"2024-11-19T08:16:56.243294Z","iopub.status.idle":"2024-11-19T08:17:27.973001Z","shell.execute_reply.started":"2024-11-19T08:16:56.243258Z","shell.execute_reply":"2024-11-19T08:17:27.971971Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Batch batch_0_100:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_1000_1100:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_100_200:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_1100_1200:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_1200_1300:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_1300_1400:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_1400_1500:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_1500_1600:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_1600_1700:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_1700_1800:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_1800_1900:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_1900_2000:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_2000_2100:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_200_300:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_2100_2200:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_2200_2300:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_2300_2400:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_2400_2500:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_2500_2540:\n  X shape: (40, 512, 512, 3)\n  y_boxes shape: (50, 4)\n  y_classes shape: (50,)\nBatch batch_300_400:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_400_500:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_500_600:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_600_700:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_700_800:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_800_900:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\nBatch batch_900_1000:\n  X shape: (100, 512, 512, 3)\n  y_boxes shape: (193, 4)\n  y_classes shape: (193,)\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras import layers, Model\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nimport os\n\ndef create_backbone(input_shape=(224, 224, 3)):\n    \"\"\"\n    Loads a pre-trained ResNet50 model as the backbone, with the top layers removed.\n\n    Args:\n        input_shape (tuple): Shape of the input images (default is (224, 224, 3)).\n\n    Returns:\n        Model: A Keras model instance representing the backbone for feature extraction.\n    \"\"\"\n    backbone = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n    backbone.trainable = False  # Freeze the backbone to retain pre-trained weights\n    return backbone\n\ndef add_detection_head(backbone, num_classes, dense_units=(512, 256), activation='relu', max_objects=10):\n    \"\"\"\n    Adds bounding box and classification layers to the backbone for ingredient detection.\n\n    Args:\n        backbone (Model): The pre-trained backbone model without top layers.\n        num_classes (int): Number of ingredient classes for classification.\n        dense_units (tuple): Number of units in each dense layer (default is (512, 256)).\n        activation (str): Activation function for dense layers (default is 'relu').\n        max_objects (int): Maximum number of objects per image.\n\n    Returns:\n        Model: The Keras model with bounding box and classification prediction layers.\n    \"\"\"\n    # Flatten backbone output\n    x = layers.Flatten()(backbone.output)\n    for units in dense_units:\n        x = layers.Dense(units, activation=activation)(x)\n\n    # Bounding box output: (batch_size, max_objects, 4)\n    bbox_output = layers.Dense(max_objects * 4, activation=\"linear\")(x)\n    bbox_output = layers.Reshape((max_objects, 4), name=\"bounding_box\")(bbox_output)\n\n    # Classification output: (batch_size, max_objects, num_classes)\n    class_output = layers.Dense(max_objects * num_classes, activation=\"softmax\")(x)\n    class_output = layers.Reshape((max_objects, num_classes), name=\"class_label\")(class_output)\n\n    # Create the final model with two outputs\n    model = Model(inputs=backbone.input, outputs=[bbox_output, class_output])\n    return model\n\n\ndef compile_model(model):\n    \"\"\"\n    Compiles the model with Adam optimizer, mean squared error loss, and mean absolute error metric.\n\n    Args:\n        model (Model): The Keras model instance to compile.\n\n    Returns:\n        Model: The compiled Keras model ready for training.\n    \"\"\"\n    # Compile the model with two loss functions and metrics\n    model.compile(\n        optimizer=Adam(),\n        loss={'bounding_box': 'mse', 'class_label': 'sparse_categorical_crossentropy'},\n        metrics={'bounding_box': 'mae', 'class_label': 'accuracy'}\n    ) # Mean Squared Error and Sparse Categorical Crossentropy for loss function and Mean Absolute Error and Accuracy for metrics\n    return model\n\ndef prepare_batch_data(X_batch, y_boxes_batch, y_classes_batch):\n    \"\"\"\n    Prepares data for batch training by grouping bounding boxes and classes per image.\n\n    Args:\n        X_batch (np.ndarray): Batch of images.\n        y_boxes_batch (np.ndarray): Batch of bounding boxes (not grouped by image).\n        y_classes_batch (np.ndarray): Batch of classes (not grouped by image).\n\n    Returns:\n        tuple: (X_batch, grouped_y_boxes, grouped_y_classes) ready for training.\n    \"\"\"\n    grouped_y_boxes = []\n    grouped_y_classes = []\n    num_images = X_batch.shape[0]\n\n    for i in range(num_images):\n        # Find bounding boxes and classes associated with this image\n        boxes = y_boxes_batch[i::num_images]  # Filter boxes for this image\n        classes = y_classes_batch[i::num_images]  # Filter classes for this image\n\n        # Ensure fixed-size output (e.g., max_objects = 10)\n        max_objects = 10\n        if len(boxes) > max_objects:\n            boxes = boxes[:max_objects]\n            classes = classes[:max_objects]\n        else:\n            # Pad with zeros if fewer objects\n            padding_boxes = np.zeros((max_objects - len(boxes), 4))\n            padding_classes = np.zeros((max_objects - len(classes),), dtype=int)\n            boxes = np.vstack([boxes, padding_boxes])\n            classes = np.hstack([classes, padding_classes])\n\n        grouped_y_boxes.append(boxes)\n        grouped_y_classes.append(classes)\n\n    return X_batch, np.array(grouped_y_boxes), np.array(grouped_y_classes)\n\n\n    return X_batch, np.array(grouped_y_boxes), np.array(grouped_y_classes)\n\n\ndef load_batch_data(output_dir, batch_id):\n    \"\"\"\n    Loads a specific batch of data from the given directory.\n\n    Args:\n        output_dir (str): Path to the directory where batches are saved.\n        batch_id (str): Identifier of the batch to load (e.g., \"batch_0_100\").\n\n    Returns:\n        tuple: Loaded batch data (X, y_boxes, y_classes).\n    \"\"\"\n    X = np.load(os.path.join(output_dir, f\"{batch_id}_X.npy\"))\n    y_boxes = np.load(os.path.join(output_dir, f\"{batch_id}_y_boxes.npy\"))\n    y_classes = np.load(os.path.join(output_dir, f\"{batch_id}_y_classes.npy\"))\n    return X, y_boxes, y_classes\n\ndef load_multiple_batches(output_dir, start_idx, num_files):\n    \"\"\"\n    Loads and concatenates multiple batches from the given directory.\n\n    Args:\n        output_dir (str): Path to the directory where batches are saved.\n        start_idx (int): Starting index of the batch files to load.\n        num_files (int): Number of files to load and concatenate.\n\n    Returns:\n        tuple: Concatenated data (X, y_boxes, y_classes).\n    \"\"\"\n    X_all, y_boxes_all, y_classes_all = [], [], []\n\n    for i in range(start_idx, start_idx + num_files):\n        batch_id = f\"batch_{i * 100}_{(i + 1) * 100}\"\n        try:\n            # Load each batch\n            X = np.load(os.path.join(output_dir, f\"{batch_id}_X.npy\"))\n            y_boxes = np.load(os.path.join(output_dir, f\"{batch_id}_y_boxes.npy\"))\n            y_classes = np.load(os.path.join(output_dir, f\"{batch_id}_y_classes.npy\"))\n\n            # Append to the overall lists\n            X_all.append(X)\n            y_boxes_all.append(y_boxes)\n            y_classes_all.append(y_classes)\n        except FileNotFoundError:\n            print(f\"Batch {batch_id} not found. Skipping...\")\n            break\n\n    # Concatenate the data along the first dimension\n    X_all = np.concatenate(X_all, axis=0)\n    y_boxes_all = np.concatenate(y_boxes_all, axis=0)\n    y_classes_all = np.concatenate(y_classes_all, axis=0)\n\n    return X_all, y_boxes_all, y_classes_all\n\n\n# Paths\nOUTPUT_DIR = \"/kaggle/working/batches\"\n\n# Create and configure the model for bounding box prediction\nbackbone = create_backbone(input_shape=INPUT_SHAPE)\nmodel = add_detection_head(backbone, num_classes=len(LABELS))\nmodel = compile_model(model)\n\n\n# Vérification\n# model.summary()\n\n# Training the model by loading batches incrementally\nepochs = 25\nnum_batches = 26\nbatch_size = 100\nvalidation_split = 0.2\n# Nouveau paramètre pour regrouper plusieurs fichiers\nfiles_per_super_batch = 3  # Combine 10 batch folders, so 1000 images\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    for i in range(0, num_batches, files_per_super_batch):\n        batch_id = f\"batch_{i * 100}_{(i + 1) * 100}\"\n        # Load a batch\n        X_train_batch, y_train_boxes_batch, y_train_classes_batch = load_batch_data(\n            output_dir=OUTPUT_DIR,\n            batch_id=batch_id\n        )\n\n        # Prepare data\n        X_train, y_train_boxes, y_train_classes = prepare_batch_data(\n            X_train_batch, y_train_boxes_batch, y_train_classes_batch\n        )\n\n        # Divide between training and validation\n        split_index = int(len(X_train) * (1 - validation_split))\n        X_val, y_val_boxes, y_val_classes = (\n            X_train[split_index:],\n            y_train_boxes[split_index:],\n            y_train_classes[split_index:],\n        )\n        X_train, y_train_boxes, y_train_classes = (\n            X_train[:split_index],\n            y_train_boxes[:split_index],\n            y_train_classes[:split_index],\n        )\n\n        # Train on a batch\n        history = model.fit(\n            X_train,\n            {'bounding_box': y_train_boxes, 'class_label': y_train_classes},\n            validation_data=(X_val, {'bounding_box': y_val_boxes, 'class_label': y_val_classes}),\n            batch_size=batch_size,  \n            epochs=1,\n            verbose=1,\n        )\n\n        # Free memory allocation\n        del X_train_batch, y_train_boxes_batch, y_train_classes_batch\n        del X_train, y_train_boxes, y_train_classes\n        del X_val, y_val_boxes, y_val_classes\n        gc.collect()\n\n\nvalidation_split = 0.2\n\nbatch_files = sorted([f for f in os.listdir(OUTPUT_DIR) if f.endswith(\"_X.npy\")])  # All batch files\nnum_batches = len(batch_files)\n\n# Save the model\nif SAVE_MODEL:\n    model_save_path = './trained_pizza_topping_model.h5'\n    model.save(model_save_path)\n    print(f\"Model saved locally at: {model_save_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T11:25:06.973363Z","iopub.execute_input":"2024-11-19T11:25:06.974002Z","iopub.status.idle":"2024-11-19T11:33:18.707440Z","shell.execute_reply.started":"2024-11-19T11:25:06.973943Z","shell.execute_reply":"2024-11-19T11:33:18.706211Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1732015516.361685     102 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - bounding_box_mae: 50.5501 - class_label_accuracy: 0.1500 - loss: 14689.8926","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1732015521.150432     102 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13s/step - bounding_box_mae: 50.5501 - class_label_accuracy: 0.1500 - loss: 14689.8926 - val_bounding_box_mae: 112.7441 - val_class_label_accuracy: 0.7600 - val_loss: 20191.3906\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 109.2862 - class_label_accuracy: 0.7475 - loss: 19122.0801 - val_bounding_box_mae: 87.3455 - val_class_label_accuracy: 0.6350 - val_loss: 13914.0449\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 82.0213 - class_label_accuracy: 0.6137 - loss: 11758.3145 - val_bounding_box_mae: 63.9199 - val_class_label_accuracy: 0.6000 - val_loss: 7820.7163\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 879ms/step - bounding_box_mae: 59.3619 - class_label_accuracy: 0.5950 - loss: 6422.3901 - val_bounding_box_mae: 48.7028 - val_class_label_accuracy: 0.6350 - val_loss: 5139.0562\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 44.7517 - class_label_accuracy: 0.6137 - loss: 4197.7661 - val_bounding_box_mae: 54.2264 - val_class_label_accuracy: 0.6450 - val_loss: 5466.4395\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 49.6277 - class_label_accuracy: 0.6125 - loss: 4327.8711 - val_bounding_box_mae: 52.0050 - val_class_label_accuracy: 0.8250 - val_loss: 5971.1772\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 46.6113 - class_label_accuracy: 0.7875 - loss: 4196.6123 - val_bounding_box_mae: 48.5712 - val_class_label_accuracy: 0.8350 - val_loss: 5891.1626\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 41.7683 - class_label_accuracy: 0.8138 - loss: 3524.8464 - val_bounding_box_mae: 49.9025 - val_class_label_accuracy: 0.8350 - val_loss: 6171.7500\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 41.6827 - class_label_accuracy: 0.8138 - loss: 3412.3521 - val_bounding_box_mae: 48.2966 - val_class_label_accuracy: 0.8350 - val_loss: 6161.4014\nEpoch 2/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 998ms/step - bounding_box_mae: 39.3991 - class_label_accuracy: 0.8138 - loss: 3302.4656 - val_bounding_box_mae: 43.3876 - val_class_label_accuracy: 0.8350 - val_loss: 5691.5688\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 34.6960 - class_label_accuracy: 0.8137 - loss: 2925.5637 - val_bounding_box_mae: 43.0053 - val_class_label_accuracy: 0.8350 - val_loss: 5183.7344\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 34.9384 - class_label_accuracy: 0.8138 - loss: 2755.1350 - val_bounding_box_mae: 43.2353 - val_class_label_accuracy: 0.8350 - val_loss: 4743.9321\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 36.4135 - class_label_accuracy: 0.8138 - loss: 2738.0483 - val_bounding_box_mae: 38.4988 - val_class_label_accuracy: 0.8350 - val_loss: 4454.4414\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 32.0533 - class_label_accuracy: 0.8137 - loss: 2596.3049 - val_bounding_box_mae: 37.0036 - val_class_label_accuracy: 0.8350 - val_loss: 4311.2900\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 30.5646 - class_label_accuracy: 0.8138 - loss: 2434.9519 - val_bounding_box_mae: 38.8396 - val_class_label_accuracy: 0.8350 - val_loss: 4359.5933\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 32.5808 - class_label_accuracy: 0.8100 - loss: 2483.6292 - val_bounding_box_mae: 38.4516 - val_class_label_accuracy: 0.8350 - val_loss: 4292.5928\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 32.2042 - class_label_accuracy: 0.8000 - loss: 2452.6650 - val_bounding_box_mae: 35.6848 - val_class_label_accuracy: 0.8350 - val_loss: 4172.3096\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 29.5040 - class_label_accuracy: 0.8000 - loss: 2354.6787 - val_bounding_box_mae: 35.1657 - val_class_label_accuracy: 0.8350 - val_loss: 4178.2935\nEpoch 3/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 28.8379 - class_label_accuracy: 0.8000 - loss: 2302.0933 - val_bounding_box_mae: 35.9606 - val_class_label_accuracy: 0.8350 - val_loss: 4289.3452\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 29.2781 - class_label_accuracy: 0.8000 - loss: 2264.8364 - val_bounding_box_mae: 35.9718 - val_class_label_accuracy: 0.8350 - val_loss: 4419.3354\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 28.8058 - class_label_accuracy: 0.8062 - loss: 2203.1846 - val_bounding_box_mae: 34.2217 - val_class_label_accuracy: 0.8350 - val_loss: 4488.6704\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 26.1727 - class_label_accuracy: 0.8038 - loss: 2089.8711 - val_bounding_box_mae: 32.0323 - val_class_label_accuracy: 0.8350 - val_loss: 4567.1162\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 23.5302 - class_label_accuracy: 0.8000 - loss: 2041.1145 - val_bounding_box_mae: 32.8169 - val_class_label_accuracy: 0.8350 - val_loss: 4558.0220\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 24.1723 - class_label_accuracy: 0.8000 - loss: 2044.0746 - val_bounding_box_mae: 33.4016 - val_class_label_accuracy: 0.8350 - val_loss: 4437.9556\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 25.0536 - class_label_accuracy: 0.8000 - loss: 2034.5457 - val_bounding_box_mae: 33.2538 - val_class_label_accuracy: 0.8350 - val_loss: 4308.4077\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 25.2580 - class_label_accuracy: 0.8000 - loss: 2021.0715 - val_bounding_box_mae: 33.0308 - val_class_label_accuracy: 0.8350 - val_loss: 4249.3765\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 25.1968 - class_label_accuracy: 0.8000 - loss: 2023.8420 - val_bounding_box_mae: 32.3126 - val_class_label_accuracy: 0.8350 - val_loss: 4221.7505\nEpoch 4/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 24.5388 - class_label_accuracy: 0.8000 - loss: 1992.4463 - val_bounding_box_mae: 30.4759 - val_class_label_accuracy: 0.8350 - val_loss: 4173.0664\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 22.6703 - class_label_accuracy: 0.8000 - loss: 1913.1028 - val_bounding_box_mae: 29.2070 - val_class_label_accuracy: 0.8350 - val_loss: 4157.7920\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 21.3786 - class_label_accuracy: 0.8000 - loss: 1873.0330 - val_bounding_box_mae: 30.1126 - val_class_label_accuracy: 0.8350 - val_loss: 4185.5864\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 22.2093 - class_label_accuracy: 0.8000 - loss: 1884.9514 - val_bounding_box_mae: 29.7189 - val_class_label_accuracy: 0.8350 - val_loss: 4194.4448\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 21.9974 - class_label_accuracy: 0.8000 - loss: 1880.2537 - val_bounding_box_mae: 29.0522 - val_class_label_accuracy: 0.8350 - val_loss: 4176.4717\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 21.6084 - class_label_accuracy: 0.8000 - loss: 1851.7312 - val_bounding_box_mae: 29.0711 - val_class_label_accuracy: 0.8350 - val_loss: 4174.9956\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 21.6033 - class_label_accuracy: 0.8000 - loss: 1843.0944 - val_bounding_box_mae: 29.1201 - val_class_label_accuracy: 0.8350 - val_loss: 4170.5137\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 21.4110 - class_label_accuracy: 0.8000 - loss: 1823.8699 - val_bounding_box_mae: 28.5587 - val_class_label_accuracy: 0.8350 - val_loss: 4176.3770\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876ms/step - bounding_box_mae: 20.5696 - class_label_accuracy: 0.8000 - loss: 1796.4635 - val_bounding_box_mae: 28.9846 - val_class_label_accuracy: 0.8350 - val_loss: 4212.3149\nEpoch 5/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 20.6440 - class_label_accuracy: 0.8000 - loss: 1789.8494 - val_bounding_box_mae: 29.6148 - val_class_label_accuracy: 0.8350 - val_loss: 4236.3457\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 21.1401 - class_label_accuracy: 0.8000 - loss: 1785.3074 - val_bounding_box_mae: 28.9659 - val_class_label_accuracy: 0.8350 - val_loss: 4207.8281\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 20.6181 - class_label_accuracy: 0.8000 - loss: 1757.6172 - val_bounding_box_mae: 27.8219 - val_class_label_accuracy: 0.8350 - val_loss: 4167.8799\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 19.8430 - class_label_accuracy: 0.8000 - loss: 1737.5925 - val_bounding_box_mae: 27.7923 - val_class_label_accuracy: 0.8350 - val_loss: 4151.3945\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 19.9739 - class_label_accuracy: 0.8000 - loss: 1732.7471 - val_bounding_box_mae: 28.0681 - val_class_label_accuracy: 0.8350 - val_loss: 4155.9805\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 20.1746 - class_label_accuracy: 0.8012 - loss: 1722.9908 - val_bounding_box_mae: 28.0327 - val_class_label_accuracy: 0.8350 - val_loss: 4169.6230\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 19.9875 - class_label_accuracy: 0.8025 - loss: 1708.2517 - val_bounding_box_mae: 28.1114 - val_class_label_accuracy: 0.8350 - val_loss: 4183.4326\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 19.9042 - class_label_accuracy: 0.8012 - loss: 1697.4792 - val_bounding_box_mae: 27.9485 - val_class_label_accuracy: 0.8350 - val_loss: 4188.0127\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 19.6266 - class_label_accuracy: 0.8000 - loss: 1683.9978 - val_bounding_box_mae: 27.5719 - val_class_label_accuracy: 0.8350 - val_loss: 4188.6494\nEpoch 6/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 19.1606 - class_label_accuracy: 0.8000 - loss: 1669.4990 - val_bounding_box_mae: 27.5260 - val_class_label_accuracy: 0.8350 - val_loss: 4188.2163\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 19.1094 - class_label_accuracy: 0.8000 - loss: 1660.2366 - val_bounding_box_mae: 27.6192 - val_class_label_accuracy: 0.8350 - val_loss: 4178.7505\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 19.1916 - class_label_accuracy: 0.8000 - loss: 1652.8215 - val_bounding_box_mae: 27.6640 - val_class_label_accuracy: 0.8350 - val_loss: 4165.0757\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 19.1657 - class_label_accuracy: 0.8000 - loss: 1641.2947 - val_bounding_box_mae: 27.5616 - val_class_label_accuracy: 0.8350 - val_loss: 4164.7617\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.9524 - class_label_accuracy: 0.8000 - loss: 1627.4531 - val_bounding_box_mae: 27.2268 - val_class_label_accuracy: 0.8350 - val_loss: 4174.3970\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.6224 - class_label_accuracy: 0.8000 - loss: 1615.7604 - val_bounding_box_mae: 27.0045 - val_class_label_accuracy: 0.8350 - val_loss: 4169.4014\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.5076 - class_label_accuracy: 0.8000 - loss: 1606.6376 - val_bounding_box_mae: 26.8551 - val_class_label_accuracy: 0.8350 - val_loss: 4142.7212\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.5010 - class_label_accuracy: 0.8000 - loss: 1595.6306 - val_bounding_box_mae: 26.8873 - val_class_label_accuracy: 0.8350 - val_loss: 4128.6836\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.5822 - class_label_accuracy: 0.8012 - loss: 1586.6835 - val_bounding_box_mae: 26.9775 - val_class_label_accuracy: 0.8350 - val_loss: 4150.5200\nEpoch 7/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.5876 - class_label_accuracy: 0.8012 - loss: 1578.2643 - val_bounding_box_mae: 27.0950 - val_class_label_accuracy: 0.8350 - val_loss: 4193.0337\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.5028 - class_label_accuracy: 0.8012 - loss: 1567.1857 - val_bounding_box_mae: 27.2169 - val_class_label_accuracy: 0.8350 - val_loss: 4226.6777\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.4562 - class_label_accuracy: 0.8012 - loss: 1556.1818 - val_bounding_box_mae: 27.1145 - val_class_label_accuracy: 0.8350 - val_loss: 4231.8687\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.3368 - class_label_accuracy: 0.8012 - loss: 1544.8672 - val_bounding_box_mae: 27.0212 - val_class_label_accuracy: 0.8350 - val_loss: 4215.4668\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1000ms/step - bounding_box_mae: 18.2966 - class_label_accuracy: 0.8000 - loss: 1535.0590 - val_bounding_box_mae: 27.1276 - val_class_label_accuracy: 0.8350 - val_loss: 4196.2612\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.3379 - class_label_accuracy: 0.8000 - loss: 1526.6053 - val_bounding_box_mae: 27.1480 - val_class_label_accuracy: 0.8350 - val_loss: 4187.5132\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.2755 - class_label_accuracy: 0.7988 - loss: 1516.8754 - val_bounding_box_mae: 27.1500 - val_class_label_accuracy: 0.8350 - val_loss: 4189.4658\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.2302 - class_label_accuracy: 0.7988 - loss: 1506.7422 - val_bounding_box_mae: 27.1982 - val_class_label_accuracy: 0.8350 - val_loss: 4189.0386\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.2154 - class_label_accuracy: 0.7988 - loss: 1497.1514 - val_bounding_box_mae: 27.1401 - val_class_label_accuracy: 0.8350 - val_loss: 4176.5898\nEpoch 8/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.1372 - class_label_accuracy: 0.7988 - loss: 1486.9445 - val_bounding_box_mae: 27.0393 - val_class_label_accuracy: 0.8350 - val_loss: 4162.1851\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.0383 - class_label_accuracy: 0.7975 - loss: 1477.1716 - val_bounding_box_mae: 27.0145 - val_class_label_accuracy: 0.8350 - val_loss: 4163.7983\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 888ms/step - bounding_box_mae: 18.0163 - class_label_accuracy: 0.7975 - loss: 1468.0140 - val_bounding_box_mae: 27.0123 - val_class_label_accuracy: 0.8350 - val_loss: 4186.0786\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.9737 - class_label_accuracy: 0.7975 - loss: 1457.3271 - val_bounding_box_mae: 27.0921 - val_class_label_accuracy: 0.8350 - val_loss: 4218.5845\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.9711 - class_label_accuracy: 0.7975 - loss: 1447.9823 - val_bounding_box_mae: 27.1276 - val_class_label_accuracy: 0.8350 - val_loss: 4237.7642\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.9370 - class_label_accuracy: 0.7975 - loss: 1438.8013 - val_bounding_box_mae: 27.1097 - val_class_label_accuracy: 0.8350 - val_loss: 4234.1089\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.8833 - class_label_accuracy: 0.7988 - loss: 1428.7660 - val_bounding_box_mae: 27.1051 - val_class_label_accuracy: 0.8350 - val_loss: 4222.7520\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.8714 - class_label_accuracy: 0.7988 - loss: 1419.2222 - val_bounding_box_mae: 27.0961 - val_class_label_accuracy: 0.8350 - val_loss: 4219.0942\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.8385 - class_label_accuracy: 0.7988 - loss: 1409.5215 - val_bounding_box_mae: 27.1558 - val_class_label_accuracy: 0.8350 - val_loss: 4226.1196\nEpoch 9/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.8090 - class_label_accuracy: 0.7988 - loss: 1399.6881 - val_bounding_box_mae: 27.2580 - val_class_label_accuracy: 0.8350 - val_loss: 4234.5537\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.7919 - class_label_accuracy: 0.7988 - loss: 1390.3041 - val_bounding_box_mae: 27.2559 - val_class_label_accuracy: 0.8350 - val_loss: 4231.7246\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.7651 - class_label_accuracy: 0.7988 - loss: 1380.4629 - val_bounding_box_mae: 27.1876 - val_class_label_accuracy: 0.8350 - val_loss: 4218.1470\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.7303 - class_label_accuracy: 0.7988 - loss: 1370.4792 - val_bounding_box_mae: 27.1408 - val_class_label_accuracy: 0.8350 - val_loss: 4211.2812\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.6864 - class_label_accuracy: 0.7988 - loss: 1360.8528 - val_bounding_box_mae: 27.1834 - val_class_label_accuracy: 0.8350 - val_loss: 4223.2739\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.6476 - class_label_accuracy: 0.7988 - loss: 1351.1282 - val_bounding_box_mae: 27.2976 - val_class_label_accuracy: 0.8350 - val_loss: 4242.0215\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.6253 - class_label_accuracy: 0.8000 - loss: 1341.4709 - val_bounding_box_mae: 27.3738 - val_class_label_accuracy: 0.8350 - val_loss: 4248.3530\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.5966 - class_label_accuracy: 0.8000 - loss: 1331.8896 - val_bounding_box_mae: 27.3508 - val_class_label_accuracy: 0.8350 - val_loss: 4243.3931\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.5482 - class_label_accuracy: 0.8000 - loss: 1322.0160 - val_bounding_box_mae: 27.2923 - val_class_label_accuracy: 0.8350 - val_loss: 4241.1904\nEpoch 10/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.4931 - class_label_accuracy: 0.8000 - loss: 1311.9821 - val_bounding_box_mae: 27.2864 - val_class_label_accuracy: 0.8350 - val_loss: 4245.3643\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876ms/step - bounding_box_mae: 17.4547 - class_label_accuracy: 0.8000 - loss: 1302.1432 - val_bounding_box_mae: 27.3046 - val_class_label_accuracy: 0.8350 - val_loss: 4251.8008\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.4267 - class_label_accuracy: 0.8000 - loss: 1292.3119 - val_bounding_box_mae: 27.3431 - val_class_label_accuracy: 0.8350 - val_loss: 4257.3330\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.4122 - class_label_accuracy: 0.8000 - loss: 1282.6289 - val_bounding_box_mae: 27.3698 - val_class_label_accuracy: 0.8350 - val_loss: 4260.0542\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.3786 - class_label_accuracy: 0.8000 - loss: 1272.8154 - val_bounding_box_mae: 27.3659 - val_class_label_accuracy: 0.8350 - val_loss: 4259.8545\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.3315 - class_label_accuracy: 0.8000 - loss: 1262.7855 - val_bounding_box_mae: 27.3718 - val_class_label_accuracy: 0.8350 - val_loss: 4260.3164\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.2869 - class_label_accuracy: 0.8000 - loss: 1252.9404 - val_bounding_box_mae: 27.4045 - val_class_label_accuracy: 0.8350 - val_loss: 4268.4102\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.2433 - class_label_accuracy: 0.8000 - loss: 1242.8787 - val_bounding_box_mae: 27.4523 - val_class_label_accuracy: 0.8350 - val_loss: 4278.7422\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.2086 - class_label_accuracy: 0.8000 - loss: 1232.9972 - val_bounding_box_mae: 27.4838 - val_class_label_accuracy: 0.8350 - val_loss: 4282.0361\nEpoch 11/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.1768 - class_label_accuracy: 0.8000 - loss: 1223.0555 - val_bounding_box_mae: 27.4662 - val_class_label_accuracy: 0.8350 - val_loss: 4277.0947\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.1247 - class_label_accuracy: 0.8000 - loss: 1212.9456 - val_bounding_box_mae: 27.4491 - val_class_label_accuracy: 0.8350 - val_loss: 4275.0386\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.0808 - class_label_accuracy: 0.8000 - loss: 1203.0485 - val_bounding_box_mae: 27.4842 - val_class_label_accuracy: 0.8350 - val_loss: 4277.9956\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.0300 - class_label_accuracy: 0.8000 - loss: 1193.0139 - val_bounding_box_mae: 27.5489 - val_class_label_accuracy: 0.8350 - val_loss: 4282.7671\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.9856 - class_label_accuracy: 0.8000 - loss: 1182.9600 - val_bounding_box_mae: 27.5594 - val_class_label_accuracy: 0.8350 - val_loss: 4288.5425\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.9466 - class_label_accuracy: 0.8000 - loss: 1171.2737 - val_bounding_box_mae: 28.1723 - val_class_label_accuracy: 0.8350 - val_loss: 4322.3828\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.2738 - class_label_accuracy: 0.8000 - loss: 1170.8105 - val_bounding_box_mae: 28.2252 - val_class_label_accuracy: 0.8350 - val_loss: 4288.1357\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 874ms/step - bounding_box_mae: 17.4448 - class_label_accuracy: 0.8000 - loss: 1161.5651 - val_bounding_box_mae: 27.9888 - val_class_label_accuracy: 0.8350 - val_loss: 4301.4614\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.9935 - class_label_accuracy: 0.8000 - loss: 1146.3365 - val_bounding_box_mae: 28.2403 - val_class_label_accuracy: 0.8350 - val_loss: 4339.6279\nEpoch 12/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880ms/step - bounding_box_mae: 17.1277 - class_label_accuracy: 0.8000 - loss: 1140.6830 - val_bounding_box_mae: 27.8978 - val_class_label_accuracy: 0.8350 - val_loss: 4339.7983\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.8516 - class_label_accuracy: 0.8000 - loss: 1125.4708 - val_bounding_box_mae: 28.1471 - val_class_label_accuracy: 0.8350 - val_loss: 4316.0679\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 17.0029 - class_label_accuracy: 0.8000 - loss: 1118.9274 - val_bounding_box_mae: 27.8423 - val_class_label_accuracy: 0.8350 - val_loss: 4307.6294\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.7904 - class_label_accuracy: 0.8000 - loss: 1105.8230 - val_bounding_box_mae: 27.8250 - val_class_label_accuracy: 0.8350 - val_loss: 4317.3892\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.6932 - class_label_accuracy: 0.8000 - loss: 1096.9739 - val_bounding_box_mae: 27.7873 - val_class_label_accuracy: 0.8350 - val_loss: 4321.3384\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.6066 - class_label_accuracy: 0.8000 - loss: 1085.0714 - val_bounding_box_mae: 27.9887 - val_class_label_accuracy: 0.8350 - val_loss: 4323.1289\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.7383 - class_label_accuracy: 0.8000 - loss: 1076.4592 - val_bounding_box_mae: 27.8291 - val_class_label_accuracy: 0.8350 - val_loss: 4319.8701\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.4352 - class_label_accuracy: 0.8000 - loss: 1063.4125 - val_bounding_box_mae: 27.9340 - val_class_label_accuracy: 0.8350 - val_loss: 4341.4771\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996ms/step - bounding_box_mae: 16.4208 - class_label_accuracy: 0.8000 - loss: 1055.0961 - val_bounding_box_mae: 27.7512 - val_class_label_accuracy: 0.8350 - val_loss: 4357.4165\nEpoch 13/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.3426 - class_label_accuracy: 0.8000 - loss: 1042.9232 - val_bounding_box_mae: 28.3324 - val_class_label_accuracy: 0.8350 - val_loss: 4383.2861\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.5188 - class_label_accuracy: 0.8000 - loss: 1036.8969 - val_bounding_box_mae: 27.8557 - val_class_label_accuracy: 0.8350 - val_loss: 4343.3384\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.1880 - class_label_accuracy: 0.8000 - loss: 1021.5438 - val_bounding_box_mae: 27.8401 - val_class_label_accuracy: 0.8350 - val_loss: 4320.4351\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.2637 - class_label_accuracy: 0.8000 - loss: 1015.0087 - val_bounding_box_mae: 27.9616 - val_class_label_accuracy: 0.8350 - val_loss: 4344.6860\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.0164 - class_label_accuracy: 0.8000 - loss: 1002.7711 - val_bounding_box_mae: 28.0294 - val_class_label_accuracy: 0.8350 - val_loss: 4345.7603\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.0736 - class_label_accuracy: 0.8000 - loss: 992.0284 - val_bounding_box_mae: 27.9418 - val_class_label_accuracy: 0.8350 - val_loss: 4354.2827\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 15.9767 - class_label_accuracy: 0.8000 - loss: 982.7235 - val_bounding_box_mae: 27.6862 - val_class_label_accuracy: 0.8350 - val_loss: 4389.0161\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 15.6785 - class_label_accuracy: 0.8000 - loss: 967.4762 - val_bounding_box_mae: 27.8713 - val_class_label_accuracy: 0.8350 - val_loss: 4372.8564\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 15.7396 - class_label_accuracy: 0.8000 - loss: 958.5109 - val_bounding_box_mae: 27.8664 - val_class_label_accuracy: 0.8350 - val_loss: 4355.7236\nEpoch 14/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 15.5995 - class_label_accuracy: 0.7988 - loss: 947.5306 - val_bounding_box_mae: 27.6540 - val_class_label_accuracy: 0.8350 - val_loss: 4398.2290\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 15.5093 - class_label_accuracy: 0.8000 - loss: 935.8007 - val_bounding_box_mae: 27.7757 - val_class_label_accuracy: 0.8350 - val_loss: 4390.2148\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 15.3506 - class_label_accuracy: 0.8000 - loss: 924.8787 - val_bounding_box_mae: 27.6978 - val_class_label_accuracy: 0.8350 - val_loss: 4370.4458\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 989ms/step - bounding_box_mae: 15.2534 - class_label_accuracy: 0.7988 - loss: 913.3539 - val_bounding_box_mae: 27.5649 - val_class_label_accuracy: 0.8350 - val_loss: 4401.5020\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 15.2394 - class_label_accuracy: 0.7988 - loss: 903.2635 - val_bounding_box_mae: 27.6079 - val_class_label_accuracy: 0.8350 - val_loss: 4408.1914\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 15.0383 - class_label_accuracy: 0.7988 - loss: 891.5984 - val_bounding_box_mae: 27.6897 - val_class_label_accuracy: 0.8350 - val_loss: 4401.7368\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 15.0372 - class_label_accuracy: 0.7988 - loss: 882.4559 - val_bounding_box_mae: 27.5603 - val_class_label_accuracy: 0.8350 - val_loss: 4404.2031\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.8862 - class_label_accuracy: 0.7988 - loss: 870.1129 - val_bounding_box_mae: 27.4741 - val_class_label_accuracy: 0.8350 - val_loss: 4416.6904\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.8071 - class_label_accuracy: 0.7988 - loss: 859.6422 - val_bounding_box_mae: 27.6904 - val_class_label_accuracy: 0.8350 - val_loss: 4426.9961\nEpoch 15/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.7243 - class_label_accuracy: 0.7988 - loss: 850.0565 - val_bounding_box_mae: 27.5857 - val_class_label_accuracy: 0.8350 - val_loss: 4442.5298\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.6067 - class_label_accuracy: 0.7988 - loss: 838.0114 - val_bounding_box_mae: 27.5294 - val_class_label_accuracy: 0.8350 - val_loss: 4439.5078\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.6403 - class_label_accuracy: 0.7988 - loss: 829.0734 - val_bounding_box_mae: 27.6645 - val_class_label_accuracy: 0.8350 - val_loss: 4432.3979\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.4582 - class_label_accuracy: 0.7988 - loss: 818.3270 - val_bounding_box_mae: 27.5704 - val_class_label_accuracy: 0.8350 - val_loss: 4462.0420\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.3937 - class_label_accuracy: 0.7988 - loss: 807.7742 - val_bounding_box_mae: 27.5003 - val_class_label_accuracy: 0.8350 - val_loss: 4446.3208\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 872ms/step - bounding_box_mae: 14.2873 - class_label_accuracy: 0.7988 - loss: 797.6681 - val_bounding_box_mae: 27.5595 - val_class_label_accuracy: 0.8350 - val_loss: 4449.6221\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.2202 - class_label_accuracy: 0.7988 - loss: 787.2565 - val_bounding_box_mae: 27.5971 - val_class_label_accuracy: 0.8350 - val_loss: 4476.5337\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.1183 - class_label_accuracy: 0.7988 - loss: 777.1794 - val_bounding_box_mae: 27.5239 - val_class_label_accuracy: 0.8350 - val_loss: 4474.0781\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.0097 - class_label_accuracy: 0.7988 - loss: 766.8257 - val_bounding_box_mae: 27.5702 - val_class_label_accuracy: 0.8350 - val_loss: 4469.5557\nEpoch 16/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.9337 - class_label_accuracy: 0.7988 - loss: 757.0590 - val_bounding_box_mae: 27.6079 - val_class_label_accuracy: 0.8350 - val_loss: 4495.7305\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.8730 - class_label_accuracy: 0.7988 - loss: 747.5018 - val_bounding_box_mae: 27.5243 - val_class_label_accuracy: 0.8350 - val_loss: 4494.8184\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.7577 - class_label_accuracy: 0.7988 - loss: 737.2225 - val_bounding_box_mae: 27.5545 - val_class_label_accuracy: 0.8350 - val_loss: 4482.6812\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.6670 - class_label_accuracy: 0.7988 - loss: 727.9114 - val_bounding_box_mae: 27.5749 - val_class_label_accuracy: 0.8350 - val_loss: 4513.1035\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.6356 - class_label_accuracy: 0.7988 - loss: 718.6069 - val_bounding_box_mae: 27.6154 - val_class_label_accuracy: 0.8350 - val_loss: 4502.9258\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.4844 - class_label_accuracy: 0.7988 - loss: 708.8400 - val_bounding_box_mae: 27.5352 - val_class_label_accuracy: 0.8350 - val_loss: 4511.2861\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.4695 - class_label_accuracy: 0.7988 - loss: 699.9979 - val_bounding_box_mae: 27.6812 - val_class_label_accuracy: 0.8350 - val_loss: 4520.7236\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.3441 - class_label_accuracy: 0.7988 - loss: 691.1310 - val_bounding_box_mae: 27.6625 - val_class_label_accuracy: 0.8350 - val_loss: 4540.1274\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.3450 - class_label_accuracy: 0.7988 - loss: 683.1223 - val_bounding_box_mae: 27.8156 - val_class_label_accuracy: 0.8350 - val_loss: 4511.9233\nEpoch 17/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.2769 - class_label_accuracy: 0.7975 - loss: 676.9882 - val_bounding_box_mae: 27.9890 - val_class_label_accuracy: 0.8350 - val_loss: 4588.8354\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.5014 - class_label_accuracy: 0.7988 - loss: 673.9041 - val_bounding_box_mae: 28.7842 - val_class_label_accuracy: 0.8350 - val_loss: 4517.8145\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.7552 - class_label_accuracy: 0.7975 - loss: 679.2832 - val_bounding_box_mae: 29.5476 - val_class_label_accuracy: 0.8350 - val_loss: 4674.3364\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.7865 - class_label_accuracy: 0.7988 - loss: 702.9807 - val_bounding_box_mae: 31.4469 - val_class_label_accuracy: 0.8350 - val_loss: 4585.7036\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.2143 - class_label_accuracy: 0.7962 - loss: 760.9634 - val_bounding_box_mae: 33.4215 - val_class_label_accuracy: 0.8350 - val_loss: 4908.6440\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.6647 - class_label_accuracy: 0.7988 - loss: 862.9578 - val_bounding_box_mae: 34.8742 - val_class_label_accuracy: 0.8300 - val_loss: 4732.9849\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 20.0230 - class_label_accuracy: 0.7900 - loss: 940.7412 - val_bounding_box_mae: 33.7421 - val_class_label_accuracy: 0.8350 - val_loss: 4934.0430\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 18.8479 - class_label_accuracy: 0.7988 - loss: 866.9289 - val_bounding_box_mae: 29.7996 - val_class_label_accuracy: 0.8350 - val_loss: 4579.6782\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.2068 - class_label_accuracy: 0.7962 - loss: 663.9276 - val_bounding_box_mae: 28.0087 - val_class_label_accuracy: 0.8350 - val_loss: 4552.0947\nEpoch 18/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 12.6185 - class_label_accuracy: 0.7975 - loss: 607.6027 - val_bounding_box_mae: 31.0764 - val_class_label_accuracy: 0.8350 - val_loss: 4813.6895\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993ms/step - bounding_box_mae: 15.8313 - class_label_accuracy: 0.7988 - loss: 719.3856 - val_bounding_box_mae: 31.8038 - val_class_label_accuracy: 0.8350 - val_loss: 4643.8076\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 16.1405 - class_label_accuracy: 0.7900 - loss: 730.5515 - val_bounding_box_mae: 28.5339 - val_class_label_accuracy: 0.8350 - val_loss: 4630.1807\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.1809 - class_label_accuracy: 0.7988 - loss: 605.4559 - val_bounding_box_mae: 28.1276 - val_class_label_accuracy: 0.8350 - val_loss: 4669.5098\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 12.6789 - class_label_accuracy: 0.7988 - loss: 586.8574 - val_bounding_box_mae: 30.5556 - val_class_label_accuracy: 0.8350 - val_loss: 4622.6284\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 14.7020 - class_label_accuracy: 0.7900 - loss: 660.7032 - val_bounding_box_mae: 29.6275 - val_class_label_accuracy: 0.8350 - val_loss: 4719.6846\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.9933 - class_label_accuracy: 0.7988 - loss: 623.2739 - val_bounding_box_mae: 27.6619 - val_class_label_accuracy: 0.8350 - val_loss: 4623.6748\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.8484 - class_label_accuracy: 0.7975 - loss: 548.4584 - val_bounding_box_mae: 29.1724 - val_class_label_accuracy: 0.8350 - val_loss: 4605.5479\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.0345 - class_label_accuracy: 0.7937 - loss: 583.3657 - val_bounding_box_mae: 29.5744 - val_class_label_accuracy: 0.8350 - val_loss: 4758.3369\nEpoch 19/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.7525 - class_label_accuracy: 0.7988 - loss: 604.1545 - val_bounding_box_mae: 28.1765 - val_class_label_accuracy: 0.8350 - val_loss: 4610.0322\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.9795 - class_label_accuracy: 0.7937 - loss: 541.3745 - val_bounding_box_mae: 28.1642 - val_class_label_accuracy: 0.8350 - val_loss: 4619.1514\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.7987 - class_label_accuracy: 0.7937 - loss: 528.7120 - val_bounding_box_mae: 29.0683 - val_class_label_accuracy: 0.8350 - val_loss: 4735.0938\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.0286 - class_label_accuracy: 0.7988 - loss: 562.7309 - val_bounding_box_mae: 28.7138 - val_class_label_accuracy: 0.8350 - val_loss: 4625.8843\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 12.2011 - class_label_accuracy: 0.7937 - loss: 536.5137 - val_bounding_box_mae: 27.7386 - val_class_label_accuracy: 0.8350 - val_loss: 4672.3330\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.2428 - class_label_accuracy: 0.7975 - loss: 499.9619 - val_bounding_box_mae: 28.3850 - val_class_label_accuracy: 0.8350 - val_loss: 4673.7666\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 12.1322 - class_label_accuracy: 0.7988 - loss: 516.9996 - val_bounding_box_mae: 28.9630 - val_class_label_accuracy: 0.8350 - val_loss: 4656.7236\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 12.0932 - class_label_accuracy: 0.7937 - loss: 518.8522 - val_bounding_box_mae: 27.7791 - val_class_label_accuracy: 0.8350 - val_loss: 4705.3237\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.2028 - class_label_accuracy: 0.7975 - loss: 485.8111 - val_bounding_box_mae: 27.8129 - val_class_label_accuracy: 0.8350 - val_loss: 4639.9282\nEpoch 20/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990ms/step - bounding_box_mae: 11.3355 - class_label_accuracy: 0.7975 - loss: 480.8878 - val_bounding_box_mae: 28.9173 - val_class_label_accuracy: 0.8350 - val_loss: 4709.3613\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.6896 - class_label_accuracy: 0.7937 - loss: 492.5499 - val_bounding_box_mae: 27.9842 - val_class_label_accuracy: 0.8350 - val_loss: 4663.1753\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.3502 - class_label_accuracy: 0.7975 - loss: 474.3730 - val_bounding_box_mae: 27.5847 - val_class_label_accuracy: 0.8350 - val_loss: 4671.3726\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.7397 - class_label_accuracy: 0.7962 - loss: 454.3919 - val_bounding_box_mae: 28.3494 - val_class_label_accuracy: 0.8350 - val_loss: 4710.8301\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.0302 - class_label_accuracy: 0.7925 - loss: 460.3851 - val_bounding_box_mae: 28.1041 - val_class_label_accuracy: 0.8350 - val_loss: 4651.2124\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.2933 - class_label_accuracy: 0.7975 - loss: 460.0634 - val_bounding_box_mae: 27.9229 - val_class_label_accuracy: 0.8350 - val_loss: 4721.1177\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.6015 - class_label_accuracy: 0.7925 - loss: 440.8503 - val_bounding_box_mae: 27.6698 - val_class_label_accuracy: 0.8350 - val_loss: 4659.2524\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.4722 - class_label_accuracy: 0.7925 - loss: 431.8185 - val_bounding_box_mae: 27.8586 - val_class_label_accuracy: 0.8350 - val_loss: 4705.4453\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.7589 - class_label_accuracy: 0.7950 - loss: 435.6295 - val_bounding_box_mae: 28.0595 - val_class_label_accuracy: 0.8350 - val_loss: 4693.3979\nEpoch 21/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.5766 - class_label_accuracy: 0.7900 - loss: 428.7807 - val_bounding_box_mae: 27.5792 - val_class_label_accuracy: 0.8350 - val_loss: 4680.3281\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.2530 - class_label_accuracy: 0.7913 - loss: 413.7302 - val_bounding_box_mae: 27.5968 - val_class_label_accuracy: 0.8350 - val_loss: 4700.2295\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.2138 - class_label_accuracy: 0.7913 - loss: 408.6974 - val_bounding_box_mae: 27.9795 - val_class_label_accuracy: 0.8350 - val_loss: 4697.4414\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.3039 - class_label_accuracy: 0.7900 - loss: 409.3156 - val_bounding_box_mae: 27.7226 - val_class_label_accuracy: 0.8350 - val_loss: 4704.8901\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.2530 - class_label_accuracy: 0.7900 - loss: 401.9686 - val_bounding_box_mae: 27.5699 - val_class_label_accuracy: 0.8350 - val_loss: 4689.3599\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.9458 - class_label_accuracy: 0.7900 - loss: 390.9463 - val_bounding_box_mae: 27.7885 - val_class_label_accuracy: 0.8350 - val_loss: 4725.2090\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.8685 - class_label_accuracy: 0.7887 - loss: 385.9321 - val_bounding_box_mae: 27.6095 - val_class_label_accuracy: 0.8350 - val_loss: 4673.3599\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.0352 - class_label_accuracy: 0.7900 - loss: 384.4410 - val_bounding_box_mae: 27.8888 - val_class_label_accuracy: 0.8350 - val_loss: 4769.3750\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.8428 - class_label_accuracy: 0.7888 - loss: 379.4849 - val_bounding_box_mae: 27.4723 - val_class_label_accuracy: 0.8350 - val_loss: 4634.2241\nEpoch 22/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.7841 - class_label_accuracy: 0.7900 - loss: 372.2107 - val_bounding_box_mae: 27.7108 - val_class_label_accuracy: 0.8350 - val_loss: 4812.8579\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.6679 - class_label_accuracy: 0.7887 - loss: 368.0554 - val_bounding_box_mae: 27.4234 - val_class_label_accuracy: 0.8350 - val_loss: 4611.3706\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.7804 - class_label_accuracy: 0.7875 - loss: 369.7081 - val_bounding_box_mae: 27.9531 - val_class_label_accuracy: 0.8350 - val_loss: 4897.2993\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.9316 - class_label_accuracy: 0.7888 - loss: 377.2547 - val_bounding_box_mae: 27.3642 - val_class_label_accuracy: 0.8350 - val_loss: 4538.4292\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 871ms/step - bounding_box_mae: 10.3884 - class_label_accuracy: 0.7862 - loss: 396.3501 - val_bounding_box_mae: 28.7769 - val_class_label_accuracy: 0.8350 - val_loss: 5154.8730\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.9582 - class_label_accuracy: 0.7862 - loss: 434.0867 - val_bounding_box_mae: 27.6614 - val_class_label_accuracy: 0.8350 - val_loss: 4397.6597\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 12.1604 - class_label_accuracy: 0.7862 - loss: 507.5797 - val_bounding_box_mae: 30.1705 - val_class_label_accuracy: 0.8200 - val_loss: 5561.9902\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 12.9225 - class_label_accuracy: 0.7862 - loss: 573.3011 - val_bounding_box_mae: 28.3369 - val_class_label_accuracy: 0.8300 - val_loss: 4324.3130\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 13.4420 - class_label_accuracy: 0.7850 - loss: 600.7593 - val_bounding_box_mae: 29.4005 - val_class_label_accuracy: 0.8200 - val_loss: 5369.9404\nEpoch 23/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.7891 - class_label_accuracy: 0.7875 - loss: 479.1352 - val_bounding_box_mae: 27.2902 - val_class_label_accuracy: 0.8250 - val_loss: 4526.6113\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.6340 - class_label_accuracy: 0.7862 - loss: 348.3373 - val_bounding_box_mae: 27.4561 - val_class_label_accuracy: 0.8350 - val_loss: 4604.4956\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.3201 - class_label_accuracy: 0.7875 - loss: 327.0731 - val_bounding_box_mae: 28.9091 - val_class_label_accuracy: 0.8200 - val_loss: 5207.0078\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.6677 - class_label_accuracy: 0.7875 - loss: 401.2068 - val_bounding_box_mae: 27.4714 - val_class_label_accuracy: 0.8300 - val_loss: 4378.1538\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 11.1457 - class_label_accuracy: 0.7850 - loss: 436.9867 - val_bounding_box_mae: 28.1116 - val_class_label_accuracy: 0.8200 - val_loss: 5061.4321\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993ms/step - bounding_box_mae: 9.8431 - class_label_accuracy: 0.7875 - loss: 362.6274 - val_bounding_box_mae: 27.6980 - val_class_label_accuracy: 0.8150 - val_loss: 4721.5527\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.1140 - class_label_accuracy: 0.7837 - loss: 314.3894 - val_bounding_box_mae: 27.6141 - val_class_label_accuracy: 0.8350 - val_loss: 4525.1895\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.8288 - class_label_accuracy: 0.7862 - loss: 338.5722 - val_bounding_box_mae: 28.6587 - val_class_label_accuracy: 0.8200 - val_loss: 5110.2607\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.8413 - class_label_accuracy: 0.7850 - loss: 350.2357 - val_bounding_box_mae: 27.9205 - val_class_label_accuracy: 0.8300 - val_loss: 4526.7505\nEpoch 24/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.1002 - class_label_accuracy: 0.7875 - loss: 337.7460 - val_bounding_box_mae: 29.4241 - val_class_label_accuracy: 0.8350 - val_loss: 4897.0117\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 10.2166 - class_label_accuracy: 0.7900 - loss: 322.6707 - val_bounding_box_mae: 28.7043 - val_class_label_accuracy: 0.8200 - val_loss: 4767.6235\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.2381 - class_label_accuracy: 0.7837 - loss: 298.0595 - val_bounding_box_mae: 28.2993 - val_class_label_accuracy: 0.8350 - val_loss: 4670.8271\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.6250 - class_label_accuracy: 0.7862 - loss: 306.9070 - val_bounding_box_mae: 28.8657 - val_class_label_accuracy: 0.8150 - val_loss: 4886.5142\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.4325 - class_label_accuracy: 0.7837 - loss: 297.7175 - val_bounding_box_mae: 27.7494 - val_class_label_accuracy: 0.8350 - val_loss: 4622.1289\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 8.7583 - class_label_accuracy: 0.7875 - loss: 277.9801 - val_bounding_box_mae: 28.5125 - val_class_label_accuracy: 0.8350 - val_loss: 4772.9604\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.2956 - class_label_accuracy: 0.7875 - loss: 280.5540 - val_bounding_box_mae: 28.4778 - val_class_label_accuracy: 0.8200 - val_loss: 4883.0273\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 8.7725 - class_label_accuracy: 0.7862 - loss: 275.8629 - val_bounding_box_mae: 28.2655 - val_class_label_accuracy: 0.8350 - val_loss: 4638.7603\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 9.2972 - class_label_accuracy: 0.7900 - loss: 281.2605 - val_bounding_box_mae: 28.3187 - val_class_label_accuracy: 0.8300 - val_loss: 4824.2817\nEpoch 25/25\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 8.7413 - class_label_accuracy: 0.7862 - loss: 267.5770 - val_bounding_box_mae: 27.9685 - val_class_label_accuracy: 0.8350 - val_loss: 4782.9702\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 8.6194 - class_label_accuracy: 0.7850 - loss: 261.8520 - val_bounding_box_mae: 27.7951 - val_class_label_accuracy: 0.8300 - val_loss: 4662.2236\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 8.5659 - class_label_accuracy: 0.7825 - loss: 259.7679 - val_bounding_box_mae: 28.0031 - val_class_label_accuracy: 0.8350 - val_loss: 4890.7080\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 8.4167 - class_label_accuracy: 0.7837 - loss: 254.6226 - val_bounding_box_mae: 27.7481 - val_class_label_accuracy: 0.8300 - val_loss: 4679.9766\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 8.3051 - class_label_accuracy: 0.7850 - loss: 250.4019 - val_bounding_box_mae: 27.4626 - val_class_label_accuracy: 0.8350 - val_loss: 4788.9834\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 7.9487 - class_label_accuracy: 0.7837 - loss: 238.7116 - val_bounding_box_mae: 28.0002 - val_class_label_accuracy: 0.8200 - val_loss: 4793.7612\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 8.0001 - class_label_accuracy: 0.7825 - loss: 237.6621 - val_bounding_box_mae: 27.3406 - val_class_label_accuracy: 0.8350 - val_loss: 4694.8892\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 7.8243 - class_label_accuracy: 0.7825 - loss: 234.4598 - val_bounding_box_mae: 27.7490 - val_class_label_accuracy: 0.8200 - val_loss: 4875.5620\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_mae: 7.8360 - class_label_accuracy: 0.7837 - loss: 231.6715 - val_bounding_box_mae: 27.6617 - val_class_label_accuracy: 0.8350 - val_loss: 4709.0649\nModel saved locally at: ./trained_pizza_topping_model.h5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 6 : Test and Visualization of Ingredient Predictions","metadata":{}},{"cell_type":"code","source":"def predict_and_plot(model, image_paths, label_colors, target_size=IMG_SIZE):\n    \"\"\"\n    Predicts bounding boxes and labels for a set of images using the trained model, \n    and plots the images with predicted bounding boxes and labels.\n\n    Args:\n        model (Model): The trained Keras model for bounding box and label prediction.\n        image_paths (list): List of paths to the test images.\n        label_colors (dict): Dictionary of colors for each label for visualization.\n        target_size (tuple): Target size to resize each image (default is IMG_SIZE).\n    \"\"\"\n    for path in image_paths:\n        # Load and preprocess image\n        img = Image.open(path)\n        img_resized = img.resize(target_size)\n        img_array = np.array(img_resized) / 255.0  # Normalize\n        \n        # Predict bounding box and class label\n        bbox_pred, class_pred = model.predict(np.expand_dims(img_array, axis=0))\n        bbox_pred = bbox_pred[0]  # Get bounding box prediction for the image\n        class_pred = np.argmax(class_pred[0])  # Get predicted class label\n        \n        # Convert bounding box to integer coordinates\n        x1, y1, x2, y2 = map(int, bbox_pred)\n        \n        # Draw bounding box and label on the image\n        fig, ax = plt.subplots(1)\n        ax.imshow(img_resized)\n        \n        # Draw bounding box\n        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n                                 edgecolor=label_colors[LABELS[class_pred]], facecolor='none')\n        ax.add_patch(rect)\n        \n        # Draw label text\n        ax.text(x1, y1 - 10, LABELS[class_pred], color=label_colors[LABELS[class_pred]], \n                fontsize=12, weight='bold', bbox=dict(facecolor='white', alpha=0.6))\n        \n        plt.axis('off')\n        plt.show()\n\nDATA_PATH = \"/kaggle/input/pizza-toppings-object-detection/Pizza Object Detector.v7i.yolov8\"\nimage_paths = os.path.join(DATA_PATH, \"valid/images\")\n\ntest_image_paths = [image_paths[i] for i in range(5)]  # Select 5 images for testing\nlabel_colors = {\n    'Arugula': 'blue', 'Bacon': 'red', 'Basil': 'yellow', 'Broccoli': 'purple', \n    'Cheese': 'cyan', 'Chicken': 'olive', 'Corn': 'magenta', 'Ham': 'teal', \n    'Mushroom': 'maroon', 'Olives': 'lime', 'Onion': 'grey', 'Pepperoni': 'navy', \n    'Peppers': 'black', 'Pizza': 'orange', 'Tomatoes': 'peach'\n}\n\nfrom tensorflow.keras.models import load_model\n\nmodel_path = \"/kaggle/working/trained_pizza_topping_model.h5\"\nmodel = load_model(model_path)\nmodel.summary()\n\npredict_and_plot(model, test_image_paths, label_colors, target_size=IMG_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T11:01:11.859577Z","iopub.execute_input":"2024-11-19T11:01:11.859991Z","iopub.status.idle":"2024-11-19T11:01:18.894893Z","shell.execute_reply.started":"2024-11-19T11:01:11.859959Z","shell.execute_reply":"2024-11-19T11:01:18.893482Z"},"trusted":true},"execution_count":22,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m     55\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/trained_pizza_topping_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 56\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     59\u001b[0m predict_and_plot(model, test_image_paths, label_colors, target_size\u001b[38;5;241m=\u001b[39mIMG_SIZE)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_api.py:183\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    177\u001b[0m         filepath,\n\u001b[1;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/legacy/saving/legacy_h5_format.py:155\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    151\u001b[0m training_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(training_config)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Compile model.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args_from_training_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    159\u001b[0m saving_utils\u001b[38;5;241m.\u001b[39mtry_build_compiled_arguments(model)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Set optimizer weights.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/legacy/saving/saving_utils.py:143\u001b[0m, in \u001b[0;36mcompile_args_from_training_config\u001b[0;34m(training_config, custom_objects)\u001b[0m\n\u001b[1;32m    141\u001b[0m loss_config \u001b[38;5;241m=\u001b[39m training_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43m_deserialize_nested_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# Ensure backwards compatibility for losses in legacy H5 files\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m _resolve_compile_arguments_compat(loss, loss_config, losses)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/legacy/saving/saving_utils.py:204\u001b[0m, in \u001b[0;36m_deserialize_nested_config\u001b[0;34m(deserialize_fn, config)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_fn(config)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    205\u001b[0m         k: _deserialize_nested_config(deserialize_fn, v)\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    207\u001b[0m     }\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    210\u001b[0m         _deserialize_nested_config(deserialize_fn, obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m config\n\u001b[1;32m    211\u001b[0m     ]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/legacy/saving/saving_utils.py:205\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_fn(config)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 205\u001b[0m         k: \u001b[43m_deserialize_nested_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeserialize_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    207\u001b[0m     }\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    210\u001b[0m         _deserialize_nested_config(deserialize_fn, obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m config\n\u001b[1;32m    211\u001b[0m     ]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/legacy/saving/saving_utils.py:202\u001b[0m, in \u001b[0;36m_deserialize_nested_config\u001b[0;34m(deserialize_fn, config)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_single_object(config):\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    205\u001b[0m         k: _deserialize_nested_config(deserialize_fn, v)\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    207\u001b[0m     }\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/losses/__init__.py:144\u001b[0m, in \u001b[0;36mdeserialize\u001b[0;34m(name, custom_objects)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.losses.deserialize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeserialize\u001b[39m(name, custom_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserializes a serialized loss class/function instance.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m        A Keras `Loss` instance or a loss function.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_OBJECTS_DICT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:575\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_objects[config], types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[0;32m--> 575\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m                \u001b[49m\u001b[43mserialize_with_public_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_module_name\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[1;32m    582\u001b[0m             serialize_with_public_class(\n\u001b[1;32m    583\u001b[0m                 module_objects[config], inner_config\u001b[38;5;241m=\u001b[39minner_config\n\u001b[1;32m    584\u001b[0m             ),\n\u001b[1;32m    585\u001b[0m             custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    586\u001b[0m         )\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PLAIN_TYPES):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:678\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    677\u001b[0m     fn_name \u001b[38;5;241m=\u001b[39m inner_config\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# Below, handling of all classes.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# First, is it a shared object?\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_object_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:812\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 812\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure custom classes are decorated with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m )\n","\u001b[0;31mTypeError\u001b[0m: Could not locate function 'mse'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'keras.metrics', 'class_name': 'function', 'config': 'mse', 'registered_name': 'mse'}"],"ename":"TypeError","evalue":"Could not locate function 'mse'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'keras.metrics', 'class_name': 'function', 'config': 'mse', 'registered_name': 'mse'}","output_type":"error"}]}]}