{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3863171,"sourceType":"datasetVersion","datasetId":2296957}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Project: Pizza Classification and Ingredient Recognition\n\n## Context\nThis project utilizes Kaggle datasets and Python notebooks running on GPUs to train two distinct models:\n1. An image classification model to detect the presence of a pizza.\n2. A model to identify the ingredients of a pizza.\n\nThe final objective is to integrate these two models to build an application capable of retrieving a pizza image, identifying if it’s a pizza, and listing the main ingredients.\n\n---\n\n## Project Steps\n\n### 1. Retrieving the \"Pizza or Not Pizza\" Dataset\n- **Objective**: Download the \"Pizza or Not Pizza\" dataset from Kaggle for training the first model.\n- **Steps**:\n  - Import the dataset via the Kaggle API.\n  - Prepare images for training (preprocessing, resizing).\n\n### 2. Training and Optimization of the \"Pizza or Not Pizza\" Model\n- **Objective**: Train a classification model to detect whether an image contains a pizza.\n- **Steps**:\n  - Define the model architecture (e.g., CNN).\n  - Use GPUs to accelerate training.\n  - Optimize hyperparameters (e.g., learning rate, number of epochs).\n\n### 3. Model Evaluation and Visualization Tool\n- **Objective**: Evaluate the model's performance and visualize some example predictions.\n- **Steps**:\n  - Calculate performance metrics (precision, recall, F1-score).\n  - Use a notebook to visualize examples of correct and incorrect predictions.\n\n### 4. Retrieving the \"Pizza Ingredients\" Dataset\n- **Objective**: Download the pizza ingredients dataset for training the second model.\n- **Steps**:\n  - Import the dataset via Kaggle.\n  - Process the data to make it compatible with the ingredient identification model.\n  \n\n### 5. Training and Optimization of the \"Pizza Ingredients\" Model\n- **Objective**: Train a model to recognize ingredients in pizza images.\n- **Steps**:\n  - Design the model for ingredient identification. In step 4, we now have imported the dataset with labels, and we then have to build the technology to put the localization boxes on these images. In that way, we have the possibilty to measure the quality of the output of the model during training, by minimizing the distance between both reality and prediction, and the value of the label.\n  - Optimize training using GPUs on Kaggle.\n  - Adjust hyperparameters to improve prediction accuracy.\n\n### 6. Test and Visualization of Ingredient Predictions\n- **Objective**: Test the ingredient model and visualize the predictions.\n- **Steps**:\n  - **Evaluation**: Calculate performance metrics such as precision, recall, and F1-score.\n  - **Prediction Visualization**: Implement an algorithm to take an image as input and return it with labeled or segmented ingredients.\n    - **Algorithm**:\n      1. Load the trained ingredient model.\n      2. Preprocess the input image (resize, normalize, etc.).\n      3. Apply the model to predict ingredients.\n      4. Create a segmentation map based on predicted ingredients.\n      5. Overlay the segmentation map on the original image.\n      6. Return the labeled image.\n  - **Example Visualization**: Display an example image with ingredient labels.\n\n### 7. Integration of Both Models\n- **Objective**: Integrate the \"Pizza or Not Pizza\" and \"Pizza Ingredients\" models for a comprehensive classification.\n- **Steps**:\n  - Create a function that first applies the pizza detection model, followed by the ingredient model.\n  - Handle cases where the image does not contain a pizza (detected as \"not pizza\").\n\n### 8. Building a Complete Example\n- **Objective**: Set up an end-to-end workflow for pizza and ingredient recognition.\n- **Steps**:\n  - Create a function to retrieve a pizza image (from a URL or by upload).\n  - Pass the image through the model pipeline to detect pizza and list ingredients.\n  - Present the final result in a notebook, with visualizations of the intermediate steps.\n\n---\n\n## Technologies and Tools\n- **Language**: Python\n- **Frameworks**: TensorFlow or PyTorch\n- **Execution Environment**: Kaggle Notebooks with GPU support\n- **Visualization**: Matplotlib, Seaborn for results display\n- **Dataset Access**: Kaggle API\n\n---\n\n## Deliverables\n- **Kaggle Notebooks**: Containing each step of the process.\n- **Trained Models**: Exported model files for pizza detection and ingredient identification.\n- **Documentation**: Explanation of each step and results.\n\n---\n\nThis project will provide hands-on experience in training image classification models and integrating them into a complete image analysis pipeline.\n","metadata":{}},{"cell_type":"markdown","source":"# Utilities for Preprocessing the \"Pizza Toppings\" Object Detection Dataset\n\nThis notebook defines and uses a set of utility functions to streamline the loading, conversion, and visualization of data for object detection. These functions are designed to efficiently prepare images and annotations from the **Pizza Toppings Object Detection** dataset, simplifying the workflow for training an object detection model with Keras and TensorFlow. Below is an overview of the primary utilities:\n\n### Utility Functions\n\n1. **`load_imgs(path, no_of_images, img_size, do_display)`**  \n   This function loads a specified number of images from a folder, resizes each image, and optionally displays them. It returns a dictionary of images and a list of their paths, standardizing image sizes for use in a deep learning model.\n\n2. **`convert_bbox(coordinates, image_width, image_height)`**  \n   This function converts bounding box coordinates from the YOLOv8 format (center coordinates and relative width/height) to a more conventional format (top-left and bottom-right corners). This conversion is essential for visualizing annotations on images and ensuring compatibility with libraries like PIL or OpenCV.\n\n3. **`load_annotations(path, labels, no_of_annotations, do_display)`**  \n   This function loads annotations from text files and associates them with the dataset’s label definitions. It processes and converts coordinates, returning a dictionary of annotations for each image. This utility links each image with its bounding box information, simplifying data preparation for model training.\n\n4. **`plot_image_annotations(image_paths, annotations, label_colors, target_size, do_display, text_display, font_scale, alpha)`**  \n   This function displays images with their annotations by drawing bounding boxes around detected objects. It uses predefined colors for each label and offers customization options like text scaling and box transparency. This utility is key for visually inspecting the annotated data and ensuring data quality before training.\n\nThese utilities provide a structured way to load, process, and handle images and annotations, making the data workflow efficient and consistent for object detection model training.\n","metadata":{}},{"cell_type":"code","source":"!pip install roboflow\n\nimport os\nimport cv2\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image, ImageDraw, ImageFont\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom roboflow import Roboflow\n\n# Global Variables\n\nINPUT_SHAPE = (512, 512, 3)\nIMG_SIZE = (512, 512)\n# Set this variable to True if you want to save the trained model locally\nSAVE_MODEL = TRUE\n\ndef load_imgs(path, no_of_images, img_size=IMG_SIZE, do_display=False):\n    \"\"\"Function which loads images from the given path\n\n    Args:\n        path (str): path to the folder with images.\n        no_of_images (int): number of images to load.\n        img_size (tuple, optional): size of the image. Defaults to IMG_SIZE.\n        do_display (bool): flag to display images. Defaults to False.\n\n    Returns:\n        images (dict): dictionary with images\n        image_paths (list): list of image paths\n    \"\"\"\n    # Declaring necessary variables\n    images = {}\n    image_paths = []\n\n    # Iterating through all images in the given path\n    for img_no, img_name in enumerate(os.listdir(path)):\n        # Breaking from loop if no_of_images is reached\n        if img_no == no_of_images:\n            break\n\n        # Loading, resizing and storing images in a dictionary\n        image_paths.append(os.path.join(path, img_name))\n        img = Image.open(os.path.join(path, img_name))\n        img = img.resize(img_size)\n        images[img_no] = img\n\n        # Displaying images if do_display is True\n        if do_display:\n            print('\\033[1m' + 'Image {}  Image name: {}'.format(img_no+1, img_name) + '\\033[0m')\n            display(img)\n    \n    # Returning images and image paths\n    return images, image_paths\n\n\ndef load_annotations(path, labels, no_of_annotations, do_display=False):\n    \"\"\"Function which loads annotations from the given path\n\n    Args:\n        path (str): path to the folder with annotations.\n        labels (dict): dictionary with labels and their corresponding string names.\n        no_of_annotations (int): number of annotations to load.\n        do_display (bool): flag to display annotations. Defaults to False.\n\n    Returns:\n        annotations (dict): dictionary with annotations.\n    \"\"\"\n    # Declaring necessary variables\n    annotations = {}\n\n    # Iterating over all files in the directory\n    for filename in os.listdir(path):\n        # Breaking from loop if no_of_annotations is reached\n        if len(annotations) == no_of_annotations:\n            break\n\n        # Displaying filename if do_display is True\n        if do_display:\n            print('\\033[35m' + 'Filename: {}'.format(filename) + '\\033[0m')\n\n        # Constructing the full file path\n        file_path = os.path.join(path, filename)\n\n        # Removing the final file extension from the filename\n        filename = filename.rsplit('.', 1)[0]\n        \n        # Declaring empty list for the current filename\n        annotations[filename] = []\n\n        # Opening the file and reading its contents\n        with open(file_path, 'r') as file:\n        # Iterating through each line in the file\n            for line in file:\n                # Splitting the line into label and coordinates\n                parts = line.strip().split(' ')\n                label = labels[int(parts[0])]\n                coordinates = [float(coord) for coord in parts[1:]]\n\n                # Storing the data in the dictionary\n                annotations[filename].append({label: coordinates})\n\n                # Displaying annotations if do_display is True\n                if do_display:\n                    print('\\033[33m' + 'Label:' + '\\033[0m' + ' {}'.format(label) + '\\033[32m' + ' Coordinates:' + '\\033[0m' + ' {}'.format(coordinates))\n        \n        if do_display:\n            print()\n\n    # Returning annotations\n    return annotations\n\ndef convert_bbox(coordinates, image_width, image_height):\n    \"\"\"Function to convert bounding box coordinates from YOLOv8 format to PIL format\n\n    Args:\n        coordinates (list): list of bounding box coordinates in YOLOv8 format [x_center, y_center, width, height]\n        image_width (int): width of the original image\n        image_height (int): height of the original image\n\n    Returns:\n        new_coordinates (list): list of bounding box coordinates in PIL format [x1, y1, x2, y2]\n    \"\"\"\n    # Unpacking coordinates from YOLOv8 format\n    x_center, y_center, width, height = coordinates\n\n    # Calculating new width and height\n    new_width = width * image_width\n    new_height = height * image_height\n\n    # Calculating new center\n    new_x_center = x_center * image_width\n    new_y_center = y_center * image_height\n\n    # Calculating new coordinates\n    new_x1 = int(new_x_center - (new_width / 2))\n    new_y1 = int(new_y_center - (new_height / 2))\n    new_x2 = int(new_x_center + (new_width / 2))\n    new_y2 = int(new_y_center + (new_height / 2))\n\n    # Returning new coordinates\n    return [new_x1, new_y1, new_x2, new_y2]\n\ndef plot_image_annotations(image_paths, annotations, label_colors, target_size, do_display=False, text_display=True, font_scale=1, alpha=0.4):\n    \"\"\"Function which plots images with annotations\n\n    Args:\n        image_paths (list): list of image paths.\n        annotations (dict): dictionary with annotations.\n        labels (dict): dictionary with annotation labels.\n        label_colors (dict): dictionary with annotation label colors.\n        target_size (tuple): target size of the image.\n        do_display (bool): flag to display images with annotations. Defaults to False.\n        text_display (bool): flag to display annotation labels. Defaults to True.\n        font_scale (float): font scale for the annotation labels. Defaults to 1.\n        alpha (float): alpha value for the bounding boxes. Defaults to 0.4.\n    \"\"\"\n    # Iterating through all the images\n    for img_no, image_path in enumerate(image_paths):\n        # Retrieving the image\n        image_path = image_paths[img_no]\n\n        # Reading the image\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, target_size)\n\n        # Retrieving the image name\n        image_name = image_path.split('/')[-1]\n        image_name = image_name.split('\\\\')[-1]\n\n        # Removing only last file extension from the image name\n        image_name = image_name.rsplit('.', 1)[0]\n\n        # Retrieving the image annotations\n        image_annotations = annotations[image_name]\n\n        # Drawing the bounding boxes around the image\n        for annotation in image_annotations:\n            # Retrieving the label and coordinates of the bounding box\n            label = list(annotation.keys())[0]\n            bbox = annotation[label]\n\n            # Converting the bounding box coordinates from YOLOv8 format to PIL format\n            bbox = convert_bbox(bbox, target_size[0], target_size[1])\n            \n            # Retrieving the coordinates of the bounding box\n            x1, y1, x2, y2 = bbox\n\n            # Creating a copy of the image\n            image_with_rectangle = image.copy()\n\n            # Creating a filled rectangle for the bounding box\n            cv2.rectangle(image_with_rectangle, (x1, y1), (x2, y2), label_colors[label], cv2.FILLED)\n\n            # Blending the image with the rectangle using cv2.addWeighted\n            image = cv2.addWeighted(image, 1 - alpha, image_with_rectangle, alpha, 0)\n\n            # Adding a rectangle outline to the image\n            cv2.rectangle(image, (x1, y1), (x2, y2), label_colors[label], 3)\n\n            # Adding the label to the image\n            if text_display:\n                # Setting the font\n                font = cv2.FONT_HERSHEY_SIMPLEX\n\n                # Drawing the label on the image, whilst ensuring that it doesn't go out of bounds\n                (label_width, label_height), baseline = cv2.getTextSize(label, font, font_scale, 2)\n\n                # Ensuring that the label doesn't go out of bounds\n                if y1 - label_height - baseline < 0:\n                    y1 = label_height + baseline\n                if x1 + label_width > image.shape[1]:\n                    x1 = image.shape[1] - label_width\n                if y1 + label_height + baseline > image.shape[0]:\n                    y1 = image.shape[0] - label_height - baseline\n                if x1 < 0:\n                    x1 = 0\n\n                # Creating a filled rectangle for the label background\n                cv2.rectangle(image, (x1, y1 - label_height - baseline), (x1 + label_width, y1), label_colors[label], -1)\n\n                # Adding the label text to the image\n                cv2.putText(image, label, (x1, y1 - 5), font, font_scale, (255, 255, 255), 2, cv2.LINE_AA)\n\n        # Displaying images with annotations if do_display is True\n        if do_display:\n            plt.figure(figsize=(10,10))\n            plt.imshow(image)\n            plt.axis('off')\n            plt.show()\n\ndef plot_model_evaluation(history):\n    \"\"\"\n    Plots the training and validation losses for bounding box and classification outputs.\n\n    Args:\n        history: History object from model training.\n    \"\"\"\n    # Plot bounding box loss\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['bounding_box_loss'], label='Train Bounding Box Loss')\n    plt.plot(history.history['val_bounding_box_loss'], label='Val Bounding Box Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Bounding Box Loss')\n    plt.legend()\n\n    # Plot classification accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['class_label_accuracy'], label='Train Classification Accuracy')\n    plt.plot(history.history['val_class_label_accuracy'], label='Val Classification Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Classification Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T09:35:34.256911Z","iopub.execute_input":"2024-11-08T09:35:34.257428Z","iopub.status.idle":"2024-11-08T09:37:31.336577Z","shell.execute_reply.started":"2024-11-08T09:35:34.257372Z","shell.execute_reply":"2024-11-08T09:37:31.335002Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a2d90d79750>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/roboflow/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a2d90d79a50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/roboflow/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a2d90d79cf0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/roboflow/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a2d90d79ea0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/roboflow/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a2d90d7a050>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/roboflow/\u001b[0m\u001b[33m\n\u001b[0m^C\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mroboflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Roboflow\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Global Variables\u001b[39;00m\n\u001b[1;32m     14\u001b[0m INPUT_SHAPE \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'roboflow'"],"ename":"ModuleNotFoundError","evalue":"No module named 'roboflow'","output_type":"error"}]},{"cell_type":"markdown","source":"# **Step 4 : Retrieving the \"Pizza Ingredients\" Dataset**","metadata":{}},{"cell_type":"code","source":"# Import librairies\nimport os\nimport zipfile\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.optimizers import Adam\nfrom kaggle.api.kaggle_api_extended import KaggleApi\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\n# Use Kaggle API\nkaggle_api = KaggleApi()\nkaggle_api.authenticate()\n\n# Download Dataset form Kaggle\nkaggle_api.dataset_download_files('matthiasbartolo/pizza-toppings-object-detection', path='./data', unzip=True)\n\n# Set up meta variables and labels definition\nimage_path = 'Pizza-Object-Detector-7/train/images'\nannotation_path = 'Pizza-Object-Detector-7/train/labels'\nLABELS = {\n    0: \"Arugula\", 1: \"Bacon\", 2: \"Basil\", 3: \"Broccoli\", 4: \"Cheese\",\n    5: \"Chicken\", 6: \"Corn\", 7: \"Ham\", 8: \"Mushroom\", 9: \"Olives\",\n    10: \"Onion\", 11: \"Pepperoni\", 12: \"Peppers\", 13: \"Pineapple\", 14: \"Pizza\", 15: \"Tomatoes\"\n}\n\n# Loading images and annotations\nimages, image_paths = load_imgs(path=image_path, no_of_images=9000, do_display=False)\nannotations = load_annotations(path=annotation_path, labels=LABELS, no_of_annotations=None, do_display=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is now imported in our workspace, we then have to reformat all the images to start the training. To do so, we will transform all the images using the PIL format. ","metadata":{}},{"cell_type":"code","source":"def preprocess_data(image_paths, annotations, target_size=(224, 224)):\n    \"\"\"\n    Preprocesses images and associated bounding box and class annotations for model training.\n\n    Args:\n        image_paths (list): List of paths to the images.\n        annotations (dict): Dictionary of bounding box annotations for each image.\n                            Keys are image names, values are lists of bounding boxes with classes.\n        target_size (tuple): Target size to resize each image (default is (224, 224)).\n\n    Returns:\n        tuple: A tuple containing:\n            - X (np.array): Array of preprocessed images, normalized to [0, 1].\n            - y_boxes (np.array): Array of bounding boxes in PIL format.\n            - y_classes (np.array): Array of class labels for each bounding box.\n    \"\"\"\n    X = []\n    y_boxes = []\n    y_classes = []\n\n    for path in image_paths:\n        # Load and resize the image\n        img = Image.open(path)\n        img = img.resize(target_size)\n        img_array = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n        X.append(img_array)\n\n        # Extract the image name to match annotations\n        image_name = os.path.basename(path).split('.')[0]\n        \n        # Check if annotations exist for this image\n        if image_name in annotations:\n            image_annotations = annotations[image_name]\n            for annotation in image_annotations:\n                label = list(annotation.keys())[0]  # Get label\n                bbox_yolo = annotation[label]  # Bounding box in YOLOv8 format\n                bbox_pil = convert_bbox(bbox_yolo, target_size[0], target_size[1])\n\n                y_boxes.append(bbox_pil)\n                y_classes.append(LABELS.index(label))  # Convert label name to class index\n\n    return np.array(X), np.array(y_boxes), np.array(y_classes)\n\n# Prepare training data\nX_train, y_train_boxes, y_train_classes = preprocess_data(image_paths, annotations)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5 : Training and Optimization of the \"Pizza Ingredients\" Model","metadata":{}},{"cell_type":"markdown","source":"## Why Use a Backbone?\n\nUsing a pre-trained backbone saves computation and time because we leverage the learned representations from another dataset (like ImageNet) rather than training a network from scratch.\n\n## Why ResNet50 as the Backbone in Our Case?\n\nIn our object detection task, we aim to identify different toppings on a pizza. ResNet50 is well-suited as a backbone because:\n1. **Feature-rich**: It is deep enough to capture complex and hierarchical features, essential for distinguishing various objects in a single image.\n2. **Efficient**: With optimized architecture, ResNet50 offers a good balance between accuracy and computational efficiency.\n3. **Transfer Learning Compatibility**: Pre-trained on ImageNet, it can transfer well to our object detection task, as it has already learned to recognize basic object shapes and patterns.\n\nBy freezing the backbone during training, we retain the valuable general features while focusing the training on the detection-specific layers (the bounding box head). This approach is efficient, prevents overfitting, and ensures the model can adapt well to our smaller dataset.\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers, Model\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_backbone(input_shape=(224, 224, 3)):\n    \"\"\"\n    Loads a pre-trained ResNet50 model as the backbone, with the top layers removed.\n\n    Args:\n        input_shape (tuple): Shape of the input images (default is (224, 224, 3)).\n\n    Returns:\n        Model: A Keras model instance representing the backbone for feature extraction.\n    \"\"\"\n    backbone = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n    backbone.trainable = False  # Freeze the backbone to retain pre-trained weights\n    return backbone\n\ndef add_detection_head(backbone, num_classes, dense_units=(512, 256), activation='relu'):\n    \"\"\"\n    Adds bounding box and classification layers to the backbone for ingredient detection.\n\n    Args:\n        backbone (Model): The pre-trained backbone model without top layers.\n        num_classes (int): Number of ingredient classes for classification.\n        dense_units (tuple): Number of units in each dense layer (default is (512, 256)).\n        activation (str): Activation function for dense layers (default is 'relu').\n\n    Returns:\n        Model: The Keras model with bounding box and classification prediction layers.\n    \"\"\"\n    # Flatten backbone output\n    x = layers.Flatten()(backbone.output)\n    for units in dense_units:\n        x = layers.Dense(units, activation=activation)(x)\n\n    # Bounding box output\n    bbox_output = layers.Dense(4, activation=\"linear\", name=\"bounding_box\")\n\n    # Classification output\n    class_output = layers.Dense(num_classes, activation=\"softmax\", name=\"class_label\")  # Softmax for multi-class classification\n\n    # Create the final model with two outputs\n    model = Model(inputs=backbone.input, outputs=[bbox_output, class_output])\n    return model\n\ndef compile_model(model):\n    \"\"\"\n    Compiles the model with Adam optimizer, mean squared error loss, and mean absolute error metric.\n\n    Args:\n        model (Model): The Keras model instance to compile.\n\n    Returns:\n        Model: The compiled Keras model ready for training.\n    \"\"\"\n    # Compile the model with two loss functions and metrics\n    model.compile(\n        optimizer=Adam(),\n        loss={'bounding_box': 'mse', 'class_label': 'sparse_categorical_crossentropy'},\n        metrics={'bounding_box': 'mae', 'class_label': 'accuracy'}\n    ) # Mean Squared Error and Sparse Categorical Crossentropy for loss function and Mean Absolute Error and Accuracy for metrics\n    return model\n\n# Create and configure the model for bounding box prediction\nbackbone = create_backbone(input_shape=INPUT_SHAPE)\nmodel = add_detection_head(backbone, num_classes=len(LABELS))\nmodel = compile_model(model)\n\n# Training the model\nhistory = model.fit(\n    X_train, \n    {'bounding_box': y_train_boxes, 'class_label': y_train_classes}, \n    epochs=10, \n    batch_size=8, \n    validation_split=0.2\n)\n\n# Call the function to plot model evaluation\nplot_model_evaluation(history)\n\n# Save the model if SAVE_MODEL is set to True\nif SAVE_MODEL:\n    model_save_path = './trained_pizza_topping_model.h5'\n    model.save(model_save_path)\n    print(f\"Model saved locally at: {model_save_path}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6 : Test and Visualization of Ingredient Predictions","metadata":{}},{"cell_type":"code","source":"def predict_and_plot(model, image_paths, label_colors, target_size=IMG_SIZE):\n    \"\"\"\n    Predicts bounding boxes and labels for a set of images using the trained model, \n    and plots the images with predicted bounding boxes and labels.\n\n    Args:\n        model (Model): The trained Keras model for bounding box and label prediction.\n        image_paths (list): List of paths to the test images.\n        label_colors (dict): Dictionary of colors for each label for visualization.\n        target_size (tuple): Target size to resize each image (default is IMG_SIZE).\n    \"\"\"\n    for path in image_paths:\n        # Load and preprocess image\n        img = Image.open(path)\n        img_resized = img.resize(target_size)\n        img_array = np.array(img_resized) / 255.0  # Normalize\n        \n        # Predict bounding box and class label\n        bbox_pred, class_pred = model.predict(np.expand_dims(img_array, axis=0))\n        bbox_pred = bbox_pred[0]  # Get bounding box prediction for the image\n        class_pred = np.argmax(class_pred[0])  # Get predicted class label\n        \n        # Convert bounding box to integer coordinates\n        x1, y1, x2, y2 = map(int, bbox_pred)\n        \n        # Draw bounding box and label on the image\n        fig, ax = plt.subplots(1)\n        ax.imshow(img_resized)\n        \n        # Draw bounding box\n        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, \n                                 edgecolor=label_colors[LABELS[class_pred]], facecolor='none')\n        ax.add_patch(rect)\n        \n        # Draw label text\n        ax.text(x1, y1 - 10, LABELS[class_pred], color=label_colors[LABELS[class_pred]], \n                fontsize=12, weight='bold', bbox=dict(facecolor='white', alpha=0.6))\n        \n        plt.axis('off')\n        plt.show()\n\n        \ntest_image_paths = [image_paths[i] for i in range(5)]  # Select 5 images for testing\nlabel_colors = {\n    'Arugula': 'blue', 'Bacon': 'red', 'Basil': 'yellow', 'Broccoli': 'purple', \n    'Cheese': 'cyan', 'Chicken': 'olive', 'Corn': 'magenta', 'Ham': 'teal', \n    'Mushroom': 'maroon', 'Olives': 'lime', 'Onion': 'grey', 'Pepperoni': 'navy', \n    'Peppers': 'black', 'Pizza': 'orange', 'Tomatoes': 'peach'\n}\n        \npredict_and_plot(model, test_image_paths, label_colors, target_size=IMG_SIZE)","metadata":{},"execution_count":null,"outputs":[]}]}